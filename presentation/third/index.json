[{"content":"aaa\n","description":"Conferencias y talleres para Asociacion Granadina de Altas capacidades","id":2,"section":"showcase","tags":null,"title":"Conferencias y talleres para Asociacion Granadina de Altas capacidades","uri":"https://thebooort.github.io/showcase/comunicaci%C3%B3n-cient%C3%ADfica/asgran/"},{"content":"aaa\n","description":"Talleres en la Feria de la ciencia del Parque de las Ciencia (Granada)","id":5,"section":"showcase","tags":null,"title":"Talleres en la Feria de la ciencia del Parque de las Ciencia (Granada)","uri":"https://thebooort.github.io/showcase/comunicaci%C3%B3n-cient%C3%ADfica/feria_de_la_ciencia/"},{"content":"Image credit: CDC Unsplash\nEstimando R_0 de Granada (ritmo de contagio) en ventanas semanales En lugar de hacer estimaciones o predicciones (aún creo que tengo que seguir leyendo más sobre el tema para lanzarme) he optado por analizar el ritmo reproductivo básico.\n(de Wikipedia) En epidemiología, el número básico de reproducción (a veces llamado ritmo básico de reproducción, ratio reproductiva básica y denotadas por $R0$, r sub-cero) de una infección es el número promedio de casos nuevos que genera un caso dado a lo largo de un período infeccioso.\nEste número es importante porque permite darnos una idea de cómo está evolucionando el virus y si está siendo útil las normas de confinamiento. Además nos ayuda a evaluar la evolución de la pandemia: cuando el numéro baje de 1 tendremos un virus que se contagia a menos de una persona, es decir que tenderá a desaparecer en el tiempo.\nAsí pues, vamos a analizar el $R_0$ en la provincia en la que resido durante esta epidemia: Granada.\nGetting the data A la hora de obtener los datos, he ido directamente a la página de la Junta de Andalucía. Es cierto que la página está un pelín atrasada, pero es la única fuente oficial que he encontrado que permite personalizar los datos para obtener los de Granada en particular.\nLa descargar de datos es bastante sencilla, y al poco tengo un csv con los casos nuevos, los ingresos en UCI y las muertes.\nEn nuestro caso, vamos a usar los casos nuevos. Evidentemente tenemos sesgos debido a que nuestro país no esta realizando test masivos, aún así, para evaluar la pandemia son interesantes.\nTambién tenemos que tener en cuenta que Granada es una ciudad típicamente universitaria, y que a poco de comenzar el confinamiento, muchos universitarios han regresado a su casa. Esto creo que también ayuda a la evolución de la enfermedad en esta provincia.\nAnalyzing the data Para anailzar los datos vamos a cambiar de Python ( mi lenguaje habitual) a R. ¿Por qué? Bueno esencialmente por comodidad. Realmente R es un lenguaje bastante sencillo de manejar y posee un gran cantidad de paquetes que hacen muy sencillo el análisis estadístico de datos de variados orígenes.\nEn nuestro caso vamos a usar EpiEstim, paquete que por ejemplo, también está usando ElPais (un periodico español) en sus análisis. EpiEstim se encuentra en los repositorios oficiales de R y podéis encontrar el artículo que lo presenta aqui: A New Framework and Software to Estimate Time-Varying Reproduction Numbers During Epidemics Anne Cori, Neil M. Ferguson, Christophe Fraser and Simon Cauchemez American Journal of Epidemiology 2013.\nEste paquete se centra en el estudio de $R_t$, esat constante es el número reproductivo instantaneo, que se puede estimar mediante el número de nuevas infecciones generadas en tiempo $t$, frente al total de infecciosidad de los individuos contagiados en tiempo $t$. Este total se calcula siguiendo un sumatorio que usa la distribución de probabilidad que sigue la enfermedad ( que suponemos normal) y la incidencia en $t-1$. En resumen, $R_t$ es la media de casos secundarios que cada individuo contagiado puede provocar si las condiciones se mantienen en tiempo $t$.\nHacer inferencia sobre esta variable puede ser complicado por su alta variabilidad. Por eso, el paquete plantea hacer inferencia de la misma en ventanas temporales que se solapan, para aportar mayor significación a las estimaciones.\nLo último que necesitamos para hacer nuestra predicción es el intervalo serial (serial interval en ingles). El intervalo serial es el nombre que recibe el tiempo que transcurre entre los casos sucesivos en una cadena de transmisión.\nHabitualmente se estima generalmente a partir del intervalo entre los inicios clínicos, en cuyo caso es el \u0026ldquo;intervalo de serie de inicio clínico\u0026rdquo; cuando estas cantidades son observables. En principio, podría estimarse por el intervalo de tiempo entre la infección y la transmisión posterior. En el caso particular del covid he usado esta referencia:\n Serial interval of novel coronavirus (COVID-19) infections. Hiroshi Nishiura, Natalie M. Linton,Andrei R. Akhmetzhanov  Con lo que el intervalo será de 4.6 días con media de 4.8 días y desviación típica de 2.3 días.\nPlot Obtenemos finalmente el gráfico. Como véis he representado 3 datos esenciales para nuestro análisis. Por una parte un histograma básico con el número de casos que han acontecido estos días (desde que fueron representativos, recordemos que Granada tuvo 0 casos hasta muy entrada la pandemia en muchas provincias).\nEn segundo lugar aparece la estimación del ritmo reproductivo básico a lo largo del tiempo. Aunque no se aprecie posee los rangos superior e inferior del mismo. Y finalmente se muestra cual es el intervalo serial escogido como hemos comentado en la sección anterior.\nConclusions! Parecen buenas noticias! Hemos pasado ya a un estado con $R_0\u0026lt;1$, aunque son buenas noticias (implicarían que a la larga la enfermedad desaparecería) hay que tener en cuenta que esto se debe a las medidas de confinamiento y que debemos ser muy cautos aún.\nDeberíamos bajar este término todo lo posible antes de acabar con el confinamiento, para reducir lo máximo posible los rebrotes.\nA\n","description":"Estimando R_0 de Granada (ritmo de contagio) en ventanas semanales","id":11,"section":"posts","tags":["Visualization","Data","COVID19","R","Statistics","Mathematics"],"title":"¿Cómo vamos? Estimando el ritmo de contagio ","uri":"https://thebooort.github.io/posts/r_0granada/"},{"content":"Image credit: Ivan Aleksic @ivalex Unsplash\n¿Cómo afecta el confinamiento a los niveles de polución? Durante la cuarentena que estamos viviendo la reducción de la movilidad de las personas es evidente.\nLigada a esta disminución del tránsito de vehiculos particulares, vehiculos de transporte público y algunas maquinarias pertenecientes a servicios que se encuentran inactivos, muchos medios se han hecho eco de la disminución en los niveles de polución ambiental. Es más, al ser COVID una enfermedad que afecta a las vías respiratorias, ya hay algunos analisis que relacionan la polución del aire con tasas de moratilidad mas altas por COVID.\nPor este motivo me pareció interesante estudiar el caso de la ciudad en la que resido: Granada(España).\nLa evidencia muestra que los niveles de polución han bajado, pero ¿cómo sabemos esto?.\nPara contestar a esto vamos a tomar dos enfoques: primero, conocer qué buscamos en el aire para mediar su calidad, y, segundo, conocer alguna técnica que nos ofrezca los datos necesarios para nuestro análisis.\n¿Qué buscamos? La calidad del aire se mide en relación a la presencia y cantidad de determinados contaminantes en el aire. Los principales contaminantes monitorizados suelen estar determinados por la legislación vigente, que también determina qué baremos se utilizan para evaluar cada uno.\nAun así, hay una gran variedad de contaminantes sobre los cuales existe un conseso (en cuanto a su peligrosidad) y que son monitorizados habitualmente en estaciones medidoras de calidad del aire (habitualmente en ciudades). Algunos ejemplos son:\n $NO_2$ y en general los óxidos de nitrógeno. Estos se producen como consecuencia del uso de combustibles fósiles como petróleo, carbón o gas natural. Es un contaminante muy perjudicial. $CO_2$ y $CO$ su origen antropogénico es debido a la combustión incompleta de materias orgánicas (gas, carbón, madera, etc.), en especial los carburantes de los automóviles.  A estos habría que sumarles otros como las particulas en suspensión ($PM_5$ o $PM_10$ según su tamaño), el Ozono, el Dióxido de azufre, etc.\nDespúes de consultar diversas páginas y leer las noticias sobre polución y COVID, me quedaba claro que sería muy interesante comprobar los niveles de NO_2, pues tienen una clara relación con las actividades de transporte y su uso está muy extendido como parámetro que mide la calidad del aire.\nGetting the data ¿Cómo obtenemos los datos? Esencialmente los datos sobre los contaminantes se pueden encontrar en internet vía dos posibles orígenes.\nPor una parte muchos de estos datos se originan en estaciones de medición locales. Estas permiten una obtención de datos directamente de nucleos urbanos particulares, con una cantidad de datos relativamente pequeña. La parte negativa es que dependes de los sensores de la estación. En mi caso, Granada tiene varias estaciones, pero por ejemplo no pude encontrar datos de Ozono o de las partículas en suspensión.\nLa otra opción es usar datos de satélite. Aunque hay datos bastante actualizados sobre estas mediciones, habitualmente pesan bastante (puesto que se parten solo regiones grandes como Europa o America del Norte), y ahora mismo mi conexión de internet no da para tanto.\nParticularizando esta última opción, si os véis valientes en Europa tenempos el TROPOMI, un instrumento de monitorización de la trposfera que tiene datos de NO2 que forma parte del Sentinel-5 de la Agencia Espacial Europea.\nFinalmente, para mi análisis me dicidí por usar datos de las estaciones locales, aunque solo encontré de NO2, al menos son datos oficiales y de un volumen manejable para tratarlos. Como extra también tuve acceso al histórico de otros años, para así compararlo. Toda la información la saqué de la Agencia Medioambiental Europea.\nAnalyzing the data La verdad es que los datos no estaban en la mejor versión posible. Además fue un poco lioso conseguirlos. Aún así, con un poco de preprocesado obtuve lo que buscaba.\nLo único a destacar durante esta fase es la conversion a dato tipo fecha que necesitas para plotear datos pertenecientes a series temporales:\n1 2  no2_df_2020[\u0026#39;date\u0026#39;] = pd.to_datetime(no2_df_2020[\u0026#39;date_time\u0026#39;]) no2_df_2019[\u0026#39;date\u0026#39;] = pd.to_datetime(no2_df_2019[\u0026#39;date_time\u0026#39;])   En este aspecto ospuede ayudar strftime, pues que en muchas ocasiones os vais a encontrar fechas en formatos muy diferentes. Es habitual tambien que algunas librerías requieran el formato de año-mes-dia , o mes-dia-año, para todas estas ocasiones con strftime podéis salir del paso.\n(El ejemplo es de otro código pero así os haceís una idea de como usarlo)\n1  df[\u0026#39;date\u0026#39;] = pd.to_datetime(df[\u0026#34;date\u0026#34;].dt.strftime(\u0026#39;%d/%m/%Y\u0026#39;))   Y bueno, como ya sabéis empecé a usar bokeh hace poco, asi que esto era una situación perfecta para poner a prueba mis conocimientos.\nPlot  He marcado con verde ( no lo había usado antes) el nivel de contaminante que la OMS considera seguro (en media al año).\nPara ello he usado la funcionalidad BoxAnnotation, que además puede personalizarse un monton y se puede usar con fechas de inicio y fin:\n1 2 3 4  low_box = BoxAnnotation(top=40, fill_alpha=0.1, fill_color=\u0026#39;green\u0026#39;) top_box = BoxAnnotation(bottom=40, top=80, fill_alpha=0.1, fill_color=\u0026#39;red\u0026#39;) p1.add_layout(low_box) p1.add_layout(top_box)   Añadí el marcaje del día en el que se inicia la cuarentena. Además saqué la leyenda fuera, porq dentro del gráfico molestaba bastante:\n1 2 3 4 5 6 7 8  from bokeh.models import Legend legend = Legend(items=[ (\u0026#34;2020\u0026#34; , [r1,r2]), (\u0026#34;2019\u0026#34; , [r3, r4]), ], location=\u0026#34;center\u0026#34;) p1.add_layout(legend, \u0026#39;right\u0026#39;)   Conclusions! Si bien es cierto que en abril hay un descenso en los dos años (puede que la lluvia ayude), veníamos de un febrero muy caluroso y sin apenas lluvia, por lo tanto veníamos de una situación negativa. También se puede observar que ya de por sí nos encontrabamos en un punto muy bueno, menor al del año pasado. Me llama la atención un pequeño repunte (quizas por el mayor uso de coches unipersonales para ir al trabajo en lugar de transporte público?), con todo, estamos lejos de los niveles del año pasado. No olvidemos tampoco que estamos representando un promedio semanal, con lo que captamos eventos que han podido pasar durante la semana completa.\nA mitad del encierro hay una gran bajada que nos deja en un nivel muy bajo y posiblemente relacionada con la mayor restricción de circulación. Llegamos a un punto muy inferior a otros años por la misma fecha, lo que parece indicar que sí que se debe a causas antropológicas. Además es un buen indice de que la gente se está tomando en serio la cuarentena :) y que aunque sea por unos días, estamos respirando un aire más limpio.\n","description":"Evolución de los niveles de polución en Granada durante la cuarentena","id":12,"section":"posts","tags":["Visualization","Data","COVID19","Python","Pollution","Ecollogy","Air Quality"],"title":"El aire que respiramos durante el confinamiento","uri":"https://thebooort.github.io/posts/pollution_levels/"},{"content":"Image credit: Javier Ezpeleta @javierezpeleta Unsplash\nDid confinement affects my sleep routine? Llevamos ya varias semanas de cuarentena y los ánimos varían mucho de un día a otro. Ultimamente han aparecido muchos artículos y estudios sobre el posible impacto del confinamiento en nuestros ritmos de vida. El tema me despertaba bastante curiosidad desde el principio, y da la casualidad de que por mis otros hobbies (trail running) tengo un reloj capaz de captar algunas de mis rutinas. En particular el reloj puede realizar mediciones de sueño, actividad física, pasos, etc.\nEvidentemente la actividad física se vió afectada por el confinamiento, reduciendo mis ejercicios y mis pasos diarios. Para que os hagáis una idea mis pasos han pasado de 14.149,2 de media diaria a sólo 1869,1.\nSi bien es cierto que cuando más ando, al salir a comprar para varios días, no me llevo el reloj para evitar tocarlo con las manos sucias, actualmente salgo 1 vez cada semana y media, con lo cual creo que no ayudan a la media en absoluto.\nTambién hay que tener en cuenta que al hacer ejercicio en casa (algo que hago entre 1 hora / 1hora y media al día), con los movimientos que practico, el reloj también cuenta pasos. Aún así, creo que gracias a eso solo he llegado un día a la cifra de 4000 pasos.\nEn resumen, creo que el parámetro del que tengo menos idea es el del sueño, y es uno de los que destacan para evaluar mi posible productividad, descanso, concentración, o incluso estado anímico.\nGetting the data Para obtener los datos tiré de GPDR y solicité mis datos a la empresa de la cual viene mi reloj deportivo. Tardarón un par de días en darme los datos, pero a su favor, gestionaron bastante bien mi solicitud.\nLos datos se distribuyen de la siguiente manera (de su página web):\n  Sueño profundo\nCuando pasas a la fase de sueño profundo, los movimientos oculares y musculares se detienen por completo. La frecuencia cardiaca y el ritmo de tu respiración disminuyen.\n  Sueño ligero\nEl sueño ligero es la primera fase del sueño. Durante la fase de sueño ligero, los movimientos oculares y la actividad muscular se ralentizan.\n  Despierto\n  Para detectar estas fases el reloj hace uso del acelerómetro y el pulsómetro. En aquellos dispositivos con oxímetro también es tenido en cuenta, pero no es mi caso.\nAnalyzing the data Finalmente con los datos obtenidos, procedo a usar bokeh para un EDA. Dentro del mismo gráfico incluyo los datos de sueño total, de sueño profundo y sueño ligero.\nAdemás de los datos en crudo, he marcado la franja de sueño saludable recomendada (7-8h). Como parte del análisis, he realizadao una pequeña regresión para intentar ver qué tendencia seguían los datos.\nSin más información lo cierto es que son pocos datos y no es del todo representativa, pero creo que se puede intuir alguna conclusión de ella.\nConclusions! Bueno viendo los datos soy optimista. He seguido de forma adecuada un rutina de sueño para evitar alterar mis ciclos y ritmos diarios.\nEn parte estoy acostumbrado a realizar mis principales entrenamientos por la tarde-noche durante la semana. Estos horarios, al mantenerlos creo que me han ayudado a llegar cansado a la hora de dormir.\nEs cierto que se intuye un leve subida de las horas de sueño, pero con menor cantidad de sueño profundo. Esto puede afectar a la calidad del sueño, pero las variaciones son tan sumamente mínimas, que no creo que tenga que preocuparme.\n","description":"Analyzing sleep patterns before and during COVID19 quarentine","id":13,"section":"posts","tags":["Visualization","Data","COVID19","Python","Machine Learning"],"title":"Did confinement affect my sleep/exercise routine?","uri":"https://thebooort.github.io/posts/analyzing-sleep-patterns-before-and-during-covid19-quarentine/"},{"content":"Bar chart race Siguiendo un poco el interés que me suscita la representación visual de datos, aprovechando la cuarentena me puse a descubrir la mejor forma de hacer una \u0026ldquo;carrera de gráficos de barras\u0026rdquo;.\nEste tipo de visualización ha cogido fama últimamente. Más que por su su utilidad a la hora de analizar los datos, por su forma atractiva y visual de presentar la información, haciéndola muy apetecible al público en general y prestándose a ofrecer videos interesantes fáciles de consumir.\nPor esta razón, y aprovechando para adquirir nuevos conocimientos que me pueden ser útiles en el futuro, me puse a replicar este tipo de visualización gráfica. El objetivo de estos posts no es solo aprender herramientas nuevas o rebuscar códigos interesantes, también ser capaz de ofrecer un gráfico final vistoso que sea consumible.\nOpciones Buscando por internet encontré varias opciones para relizar este tipo de gráficos.\n Python: Python siempre sale a relucir sea cual sea tu requerimiento. En esta ocasión encontré posts en TowardsDataScience donde te explicaban paso a paso como conseguir el efecto. Aún así, los resultados que veía, si bien per se eran interesantes, no llegaban al resultado final que estaba buscando. Tableau: Tableau es otro clásico de la visualización utilizado por muchas empresas con una opción gratuita en Tableau Public con un gran potencial para realizar gráficos interactivos de todo tipo y con buen acabado. Algunos de los ejemplos que ví con esta herramienta no me llegaron a captar la atención, por lo que decidí dejar su aprendizaje para más tarde (tengo pendiente ponerme con él). Flourish: Llegamos finalmente a Flourish. Flourish es otra herramienta que se puede gestionar online que ofrece muchas y variadas visualizaciones. Encontré esta herramienta debido a una de las gráficas que se publicaron en un periódico online. Ojeándola me gustó mucho su acabado y decidí darle una oportunidad para el objetivo que tenía.  Datos y preprocesado Una vez que escogí Flourish, quedaba elegir qué datos quería representar. Como llevaba tiempo trabajando con casos de COVID-19 decidí alejarme un poco de esta dinámica y buscar alguna fuente de datos interesante.\nCasi por casualidad ese día estuve comentando los datos sobre las disminuciones de contaminación atmosférica de las grandes ciudades debido al confinamiento. Asi que las emisiones de $CO_2$ rondaban mi mente, y aunque había relación con el COVID19, el aspecto ecológico me atraía bastante.\nEncontrar dataset buenos sobre cualquier tema es complicado, más aún temás ecológicos/climáticos. Por suerte en ensta ocasion cuento con OurWorldInData fuente de datasets de todo tipo con muchisima información y bien formateados. Decidí elegir las emisiones de $CO_2$ por persona. Esencialmente, para calcular la contribución del ciudadano promedio de cada país a las emisiones totales de $CO_2$ se dividen sus emisiones totales por su población, obteniendo las medidas en tonelaadas por usuario por año.\nEstas emisiones juegan un papel fundamental dentro de los objetivos climáticos de los años venideros y, como dicen en la web: gracias a las reconstrucciones históricas, están disponibles para todo el mundo desde mediados del siglo XVIII.\nEsto nos permite captar tendencias globales, casos concretos con cifras muy altas y analizar quienes son los principales emisores de $CO_2$\nUna vez descargamos los datos y los exportamos a Flourish, tenemos un par de problemas:\n  Formato de los datos: Los datos se encuentran por fecha y pais, pero, para repesentarlos como queremos necesitamos que se muestren como una serie temporal. Con ese fin, simplemente mapeamos los distintos años y creamos columnas con cada país.\n  Datos perdidos y nuevos paises: Algunos países no poseen todos los datos, en estos casos hemos optado por hacer una media entre el año posterior del que tenemos datos y el año anterior del cual tenemos datos. No es una solución ideal, pero como este tipo de gráfico no acaba representando toda la información, mas tarde nos cercioramos de que ninguno de los países en los cuales hicimos este procesado sale en el gráfico final. Para aquellos países que no tienen datos hasta un determinado año, la información se completa con ceros.\n  Una vez solventados estos dos problemas, toqueteamos las opciones de Flourish hasta que nos convenza el diseño.\nResultado final Grafico interactivo Este es el resultado final. Un gran acabado, muy contento con el resultado ¡y es interactivo!\n Video Este tipode gráficos también quedan vistosos en video, aquí os dejo el resultado con musiquita:\n  Conclusiones curiosas Por un lado destacan aquellos paises que son grandes productores de crudo, sobre todo aquellos con una población pequeña (Qatar , Kuwait\u0026hellip;). Por otra parte destaca un pico en Brunei a mediados del siglo XX, este pico da para mucho y debería escribir un post sobre él.\nDicho esto, creo que los resultados son mas que satisfactorios, una herramienta interesante y habilidades de manejo que pueden serme útiles si necesito tirar de graficos animados e interactivos con formatos que otras librerías no tienen (o cuesta mucho implementar).\n","description":"Probando Flourish para hacer graficos dinámicos","id":14,"section":"posts","tags":["Flourish","Visualization","Data","Bar Chart Race"],"title":"CO₂ emissions per capita in history: a bar chart race","uri":"https://thebooort.github.io/posts/bar_chart/"},{"content":"COVID19 Kaggle Challenge Dentro de los challenges lanzados por Kaggle para usar los datos publicados del COVID19, los doctorandos que compartimos directora de tesis nos juntamos para probar ideas y aprender lo que pudiésemos en el camino.\nHabía varios retos o tareas en las que podíamos embarcarnos, pero como nuestro interés era más bien académico, decidimos empezar haciendo un análisis exploratorio de datos y clusterización. Y desde ahí, lo que quisiésemos o nos diese tiempo.\nLos resultados que véis aquí corresponden a un enfoque muy sencillo y básico de aplicación de técnicas de machine learning para crear un recomendador de artículos científicos relacionados con uno seleccionado por el usuario.\nEl objetivo es usar los titulos y abstracts de los artículos para extraer \u0026ldquo;de qué van\u0026rdquo;. Una vez obtenido esto, se buscan artículos que traten temas similares.\nPara ello seguimos estos pasos:\n EDA Text Preprocessing Tokenization vectorization LDA and Topic Extraction Recommendation system  Si quereis echarle un vistazo a el notebook (aunque esta desordenado): https://www.kaggle.com/thebooort/covid19-exploration-and-paper-recommendation\nEste post pretende presentar los principales resultados del notebook (hasta que encuentre una forma de subir los notebooks enteros), omitiré trozos de código que considere poco importantes.\nEDA En este aspecto, trabajando con abstract y texto, tendremos que saber cuantas palabras realmente van a entrar a nuestro sistema. Para ello una diagramas de cajas puede sernos útil:\n1 2 3 4 5  df[[\u0026#39;abstract_word_count\u0026#39;]].plot(kind=\u0026#39;box\u0026#39;, title=\u0026#39;Boxplot of Word Count\u0026#39;, figsize=(10,6)) plt.show() df[[\u0026#39;body_word_count\u0026#39;]].plot(kind=\u0026#39;box\u0026#39;, title=\u0026#39;Boxplot of Word Count\u0026#39;, figsize=(10,6)) plt.show()   Además, una nube de palabras tambien puede darnos una idea aproximada de qué palabras tienen importancia (tanto aquellas que son importante como las que no).\n1 2 3 4 5 6 7 8  for key in [\u0026#39;abstract\u0026#39;,\u0026#39;title\u0026#39;,\u0026#39;full_text\u0026#39;]: total_words = df[key].values wordcloud = WordCloud(width=1800, height=1200).generate(str(total_words)) plt.figure( figsize=(30,10) ) plt.title (\u0026#39;Wordcloud\u0026#39; + key) plt.imshow(wordcloud) plt.axis(\u0026#34;off\u0026#34;) plt.show()   Tokenización Para esta seccion siguiendo un código de ajrwhite, usamos scapy. La idea es tokenizar el texto, no solo para esta parte, si no para cualquier analisis o algoritmo posterior, quedandonos con un texto depurado y palabras que realmente sean importante para etiquetar, clasificar, o mdelizar estos textos.\n1 2 3 4 5  # importamos scispacy un paquete para trabajar spaCy con textos científicos y usamos el modelo # en_core_sci_md que contine pipeline de spacy especifica para textos biomédicos (50k palabras) import en_core_sci_md nlp = en_core_sci_md.load(disable=[\u0026#34;tagger\u0026#34;, \u0026#34;parser\u0026#34;, \u0026#34;ner\u0026#34;]) nlp.max_length = 2000000   Del word cloud tenemos algunas sugerencias de stop words que no son relevantes en nuestro análisis.\nAñadiéndolas lanzamos finalmente la tokenizacion y la aplicamos sobre nuestras columnas de texto.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  customize_stop_words = [ \u0026#39;doi\u0026#39;, \u0026#39;preprint\u0026#39;, \u0026#39;copyright\u0026#39;, \u0026#39;peer\u0026#39;, \u0026#39;reviewed\u0026#39;, \u0026#39;org\u0026#39;, \u0026#39;https\u0026#39;, \u0026#39;et\u0026#39;, \u0026#39;al\u0026#39;, \u0026#39;author\u0026#39;, \u0026#39;figure\u0026#39;, \u0026#39;rights\u0026#39;, \u0026#39;reserved\u0026#39;, \u0026#39;permission\u0026#39;, \u0026#39;used\u0026#39;, \u0026#39;using\u0026#39;, \u0026#39;biorxiv\u0026#39;, \u0026#39;fig\u0026#39;, \u0026#39;fig.\u0026#39;, \u0026#39;al.\u0026#39;, \u0026#39;di\u0026#39;, \u0026#39;la\u0026#39;, \u0026#39;il\u0026#39;, \u0026#39;del\u0026#39;, \u0026#39;le\u0026#39;, \u0026#39;della\u0026#39;, \u0026#39;dei\u0026#39;, \u0026#39;delle\u0026#39;, \u0026#39;una\u0026#39;, \u0026#39;da\u0026#39;, \u0026#39;dell\u0026#39;, \u0026#39;non\u0026#39;, \u0026#39;si\u0026#39;,\u0026#39;elsevier\u0026#39;, \u0026#39;unrestricted\u0026#39;, \u0026#39;grant\u0026#39;, \u0026#39;repository\u0026#39;, \u0026#39;original\u0026#39;, \u0026#39;acknowledgement\u0026#39;, \u0026#39;permission\u0026#39;, \u0026#39;centre\u0026#39;] for w in customize_stop_words: nlp.vocab[w].is_stop = True stop_words = nlp.Defaults.stop_words # definimos el tokenizador con las stopwords  def spacy_tokenizer(sentence): return [word.lemma_ for word in nlp(sentence) if not (word.like_num or word.is_stop or word.is_punct or word.is_space or len(word)==1)] # aplicar tokenizacion df[\u0026#39;full_text\u0026#39;] = df[\u0026#39;full_text\u0026#39;].apply(spacy_tokenizer) df[\u0026#39;abstract\u0026#39;] = df[\u0026#39;abstract\u0026#39;].apply(spacy_tokenizer) df[\u0026#39;title\u0026#39;] = df[\u0026#39;title\u0026#39;].apply(spacy_tokenizer) # eliminar espacios df[\u0026#39;full_text\u0026#39;] = df[\u0026#39;full_text\u0026#39;].apply(lambda x: \u0026#39;\u0026#39;.join(x)) df[\u0026#39;abstract\u0026#39;] = df[\u0026#39;abstract\u0026#39;].apply(lambda x: \u0026#39;\u0026#39;.join(x)) df[\u0026#39;title\u0026#39;] = df[\u0026#39;title\u0026#39;].apply(lambda x: \u0026#39;\u0026#39;.join(x))   Vectorización Vamos a vectorizar nuestra información textual ( esto es, transformar las palabras en vectores numéricos). Para ello podemos usar CountVectorizer o TF-IDF entre otros. Usé estos dos por simpleza, pero CountVectorizer me dió mejores resultados (CountVectorizer esencialmente vectoriza la palabra contando la frecuencia de repetición de esa palabra en los títulos o en los abstracts).\nImportando sklearn usar cualquiera de estos dos algoritmos es muy sencillo. Además, podemos añadir el tokenizador que queramos, en este caso el que hemos construido con spacy.\n1 2 3 4  features = 10000 tf_vectorizer = CountVectorizer(max_features=features,tokenizer = spacy_tokenizer, min_df=10) X_tf = tf_vectorizer.fit_transform(df[\u0026#39;abstract\u0026#39;]) tf_feat_name = tf_vectorizer.get_feature_names()   LDA LDA es basciamente un método estadístico que busca encontrar una combinación lineal de rasgos que caracterizan o separan dos o más clases de objetos o eventos. La combinación resultante la podemos utilizar para separar nuestros textos en grupos y obtener qué palabras aparecen con más frecuencia dentro de esos grupos, a fin de caracterizarlos.\n1 2 3  topics = 7 lda_model = LatentDirichletAllocation(learning_method=\u0026#39;online\u0026#39;,random_state=20,n_components=topics) lda_output =lda_model.fit_transform(X_tf)   Resultado final del LDA Para visualizar LDA a parte de obtener los topics, usé pyLDAvis. pyLDAvis es una biblioteca de Python para visualización interactiva de topic models (basada en el paquete R de Carson Sievert y Kenny Shirley).\nComo dicen en su web pyLDAvis está diseñado para ayudar a los usuarios a interpretar los topics en un modelo de topics que se ha ajustado a un corpus de datos textuales. El paquete extrae la información de un modelo de topics LDA ajustado, para ofrecer una visualización interactiva.\nOs lo recomiendo.\n1 2 3  # preparing for plotting pyLDAvis %matplotlib inline pyLDAvis.enable_notebook()   La visualización está diseñada para ser utilizada en cualquier notebook, pero también se puede guardar en un archivo HTML independiente para compartir fácilmente:\n1 2 3 4  pyLDAvis.sklearn.prepare(lda_model, X_tf, tf_vectorizer) # if you want to save it  # P=pyLDAvis.sklearn.prepare(lda_model, X_tf, tf_vectorizer) # pyLDAvis.save_html(p, \u0026#39;lda.html\u0026#39;)    Podéis consultar el resultado como una página aparte aqui: https://thebooort.github.io/covizzz19/ , así os quitáis de problemas si estáis viendo este blog como theme oscuro.\nVisualizando las temáticas Las temáticas finales obtenidas fueron:\nTopic 0 ['protein', 'rna', 'proteins', 'activity', 'cell', 'replication', 'antiviral'] Topic 1 ['infection', 'cell', 'immune', 'infected', 'mice', 'response', 'induced', 'expression'] Topic 2 ['respiratory', 'pcr', 'samples', 'detection', 'viruses', 'assay', 'using', 'positive'] Topic 3 ['diseases', 'disease', 'review', 'development', 'health', 'research', 'based', 'new', 'use', 'infectious'] Topic 4 ['patients', 'calves', 'il', 'group', 'associated', 'days', 'cats', 'clinical', 'age'] Topic 5 ['cov', 'sars', 'protein', 'coronavirus', 'sequence', 'mers', 'human', 'genome', 'viruses'] Topic 6 ['health', 'influenza', 'risk', 'data', 'cases', 'disease', 'outbreak'] Podemos estudiar las correlaciones entre las puntuaciones de los topics de cada paper:\n1 2 3 4  # plotting results import seaborn as sns sns.heatmap(abs(ldadf[columns].corr()),annot=True) plt.title(\u0026#34;Correlation Plot\u0026#34;)   Obtenemos una correlación baja entre topics, lo que implica que nuestra clusterizacion (si bien mejorable) no es mala.\nRecomendaciones Para obtener la recomendacion final, bastará con comparar las componentes que identifican a cada paper. Seleccionado uno cualquiera, calcularemos la distancia con el resto de papers y devolveremos los mas cercanos. Esta distancia la podemos definir como queramos: RMSE, Cosine similarity, etc.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  from sklearn.metrics.pairwise import cosine_distances from math import sqrt def paper_recommendation_tool(selected_paper_id): selected_paper = df_total[(df_total[\u0026#39;paper_id\u0026#39;]==selected_paper_id)] dominant_topic = int(selected_paper[\u0026#39;Major_topic\u0026#39;]) # we create another df with papers evaluation recommended_papers = df_total recommended_papers = recommended_papers.drop(selected_paper.index) x= selected_paper[columns].values.tolist() y= recommended_papers[columns].values.tolist() for indice_fila, fila in recommended_papers.iterrows(): distance = cosine_distances(x[0],y[indice_fila-1]) recommended_papers[\u0026#39;distance\u0026#39;] = distance recommended_papers = recommended_papers[[\u0026#39;paper_id\u0026#39;,\u0026#39;title\u0026#39;,\u0026#39;distance\u0026#39;,\u0026#39;Major_topic\u0026#39;]].sort_values(\u0026#39;distance\u0026#39;).head(10) return dominant_topic, recommended_papers   Probamos un caso concreto\n1 2 3 4  # Example: we are interested in 4602afcb8d95ebd9da583124384fd74299d20f5b, because his use of SPINT2 dominant_topic, recommended_papers = paper_recommendation_tool(\u0026#39;4602afcb8d95ebd9da583124384fd74299d20f5b\u0026#39;) print(dominant_topic) print(recommended_papers)   ","description":"COVID19 Kaggle Challenge: Construyendo un recomendador de artículos sobre COVID19","id":15,"section":"posts","tags":["Kaggle","Coding","Data Science","COVID19","Python","Machine Learning","Recommedation Systems"],"title":"COVID19 research paper recommendation engine","uri":"https://thebooort.github.io/posts/covid_kaggle/"},{"content":"¡Ojo! Perl6 ya \u0026ldquo;no existe\u0026rdquo;, oficalmente el lenguaje ahora se llama ¡Raku! :)\nPerl6 Documentation Durante un breve periodo de tiempo estuve estudiando la evolución en las contribuciones a los repositorios de código. En muchas ocasiones se dice que este tipo de contribuciones siguen leyes de potencias, pero habitualmente esta afirmación va apoyada únicamente por un par de fits de distribuciones a los datos, sin contraste de hipótesis o comparación con otras posibles distribuciones. Aunque ya me meteré en este costal más adelante, lo cierto es que analizar las contribuciones a cualquier repositorio es siempre interesante.\nY como ya sabéis, el tema de la visualización me gusta bastante, por lo que, documentándome para el trabajo que antes os mencionaba, encontré Gource.\nY como por aquel entonces también estaba escribiendo cosillas para la documentación de Perl6 (en particular su sección sobre simulación de Ecuaciones diferenciales) decidí probar a ver que pasaba al usar esta herramienta con ese repo.\nLos resultados gustaron mucho, y subieron el video a los updates semanales de Perl6 :).\nOs animo a usar Gource en vuestros proyectos de software libre, es un curioso gráfico que puede llamar la atención de los usuarios y ser útil a la hora de mostrar vuestro repo (o la actividad en el mismo).\nVideo   Data Source Repositorio de la documentacion de Perl6 a fecha de creación del video.\nSoftware usado  Gource  ","description":"Video sobre la evolución del repo de documentación de Perl6","id":16,"section":"posts","tags":["Github","Repository","Visualization","Perl6","Raku"],"title":"How to visualize a github repository evolution in time","uri":"https://thebooort.github.io/posts/perl6-viz/"},{"content":"Este post continúa la serie sobre visualización del COVID-19. La motivación para hacerla no es otra que ocupar los ratos muertos de la cuarentena para aprender a crear gráficos interactivos con Bokeh.\nY de paso ofrecer un vistazo objetivo a los datos que nos llegan sobre la enfermedad. Sé que suena un poco raro pero ya que voy a estar dándole vueltas al tema, tener este enfoque objetivo me ayuda a sobrellevarlo.\nEn cuanto depure los códigos los subo a github y tendréis el enlace aquí:\nCOVID19 - Gráficos de Europa Para cerrar (por ahora) esta introducción que me estoy haciendo a la biblioteca Bokeh, quería aportar luz sobre la evolución de la enfermedad por el mundo.\nNi por estudios ni por trabajo, nunca me ha tocado trabajar con datos geográficos. Es por eso que quería llenar un poco la laguna que tengo a la hora de representarlos gráficamente. He visto que bokeh es un excelente recurso para eso, asi que me he lanzado a sacarle partido.\nEn esta ocasión estoy siguiendo un tutorial de Shivangi Patel.\nEl tutorial está estupendo y sirve no solo como introducción para plotear mapas si no también para cómo hacerlo en Bokeh y como usar Bokeh para generar un minidashboard en el que los datos vayan actualizandose.\nEsto último, requiere de un server que lamentable mente no tengo, asi que os dejo un su lugar un video del resultado.\nPor otra parte, he aprovechado para trabajar con geopandas y empaparme un poco del tema de datos geográficos. Lamentablemente el dataset consultado usaba un GEOID un tanto raro, que no he conseguido acoplar correactamente al archivo shapes (no usa el estandar de codificacion de países de 3 letras, y no coincide con los que he visto de 2 letras), con lo que la info de algunos países a pesar de tenerla, no está visible.\nEn cualquier caso, como introducción, contento con los resultados.\nGraficos Gráfico interactivo de casos y muertes Aquí aproveche para manejar pestañas y ofrecer en el mismo grafico la posibilidad de consultar ambas variables en el mundo. Probé con los hover, pero no me encajaron mucho y decidí que esta era la mejor opción.\n Gráfico interactivo de casos y muertes Finalmente añadí la interacción al gráfico de manera que se acualice a cada paso. La verdad que el data set ha mostrado ser insuficiente, con muchos valores perdidos. No obstante, creo que el caso de Italia (podéis hacer zoom :) ) se ve bastante bien como empeora el color.\nOs dejo por aquí el video del resultado\n  Extra: El dashboard de la Johns Hopkins University Por último aprovecho para dejaros el dashboard de la Johns Hopkins University, que está muy currado y con la info al día.\n Data Source Todos los datos están obtenidos de ECDC COVID-19\nLibrerías usadas  Bokeh Pandas GeoPandas  Con esta parte creo que aparco de momento esta serie de COVID+Bokeh en pos de hacer algún post sobre otras herramientas. No obstante, no descarto volver en breve, el tema de la visualización me gusta mucho. ","description":"Graficos sobre COVID19 usando bokeh","id":17,"section":"posts","tags":["COVID19","Python","Visualization"],"title":"COVID19 + Bokeh - Mapas!","uri":"https://thebooort.github.io/posts/covid_bokeh_mapa/"},{"content":"Este post continúa la serie sobre visualización del COVID-19. La motivación para hacerla no es otra que ocupar los ratos muertos de la cuarentena para aprender a crear gráficos interactivos con Bokeh.\nY de paso ofrecer un vistazo objetivo a los datos que nos llegan sobre la enfermedad. Sé que suena un poco raro pero ya que voy a estar dándole vueltas al tema, tener este enfoque objetivo me ayuda a sobrellevarlo.\nEn cuanto depure los códigos los subo a github y tendréis el enlace aquí:\nCOVID19- Madrid vs. Andalucía Para este segundo post quise usar los datos del crecimiento en Madrid, para compararlos con Andalucía.\nLa idea aquí era ver cómo de diferentes habían sido las escaladas de casos y ver cómo quedaban al representarlas la una frente a la otra.\nDe paso, hacer hincapié en opciones para personalizar. En particular me gusta mucho la opción \u0026lsquo;mute\u0026rsquo; de la lenyenda, que te permite cambiar la opacidad de una de las líneas para visualizar mejor el resto. Os animo a probarla.\n1 2 3 4 5 6  from bokeh.plotting import figure p = figure() p.legend.click_policy=\u0026#39;mute\u0026#39; # recordad que podéis personalizar cómo actúa en vuestra gráfica p.circle(x=\u0026#39;date\u0026#39;, y=\u0026#39;cases\u0026#39;, muted_alpha = 0.2)   Aún no manejo los hovers bien (¿quizás es problema de cómo está estructurado el dataset?), asi que estoy optando por poner toda la información. Aunque repito que no me gusta el resultado.\nPor otra parte también quise ensayar la unión de dos gráficas, que quizás se veía mas explicativa. Y ojo que han cambiado la orden en Bokeh 2.0.0\n1 2 3 4 5 6  from bokeh.plotting import figure from bokeh.layouts import row p = figure() p1 = figure() # y aqui la magia show(row(p,p1))   La verdad que es bastante intuitivo. Aunque no esté 100% contento con las gráficos, aprender la librería, hacer cositas mas o menos chulas, y poder tirar de ella en el futuro, pinta muy bien.\nLo cierto es que, además, le estoy cogiendo tirria al dark mode XD.\nGráficos Madrid  Andalucía  M vs A (misma gráfica)  M vs. A (gráficas diferentes) Aquí estña el último! Creo que es el que mejor ha quedado, donde he puesto mejor los hovers, y además he ocultado la toolbar si no estás sobre el gráfico!\n Data Source Todos los datos están obtenidos de Numeroteca COVID-19\nLibrerías usadas  Bokeh Pandas  ","description":"Graficos sobre COVID19 usando bokeh","id":18,"section":"posts","tags":["COVID19","Python","Visualization"],"title":"COVID19 + Bokeh: Madrid vs. Andalucía","uri":"https://thebooort.github.io/posts/covid_bokeh_madrid_vs_and/"},{"content":"Este post comienza una serie sobre visualización del COVID-19. La motivación para hacerla no es otra que ocupar los ratos muertos de la cuarentena para aprender a crear gráficos interactivos con Bokeh.\nY de paso ofrecer un vistazo objetivo a los datos que nos llegan sobre la enfermedad. Sé que suena un poco raro pero ya que voy a estar dándole vueltas al tema, tener este enfoque objetivo me ayuda a sobrellevarlo.\nEn cuanto depure los códigos los subo a github y tendréis el enlace aquí:\nCOVID19- El caso de Andalucía Comenzamos con el gráfico que primero vino a mi cabeza. Mi idea era obtener los datos de las provincias andaluzas, pero, hasta la fecha de subir este post, no encontraba más que los de las comunidades autónomas.\nEn este caso, el ejercicio me llevó a comenzar de cero con Bokeh. Un gráfico tan sencillo como ese sirve para iniciarte en los aspectos básicos de la librería, de los cuales destaco:\n  Hovers: son las capas de visualización que te dan información sobre un punto cuando pasas el raton por encima\n  Curdoc: importada desde bokeh.io, te permite cambiar el estilo del grafico (a modo dark jeje)\n  Manejo de TOOLS: que indica que herramientas quieres que presente el gráfico interactivo para ser manipulado. En mi caso:\n  1 2  TOOLS = \u0026#39;crosshair,save,pan,box_zoom,reset,wheel_zoom\u0026#39; plot = figure(tools=TOOLS)    Plot de datos temporales: Hubo que hacer algún reajuste sobre el tipo de datos y luego indicarle a bokeh que estabamos trabajando con esto en su creacion:  1  plot = figure(x_axis_type=\u0026#39;datetime\u0026#39;, tools=TOOLS)   Finalmente opté por crear mi primer gráfico con puntos y líneas.\nGraficos  Data Source Todos los datos están obtenidos de Numeroteca COVID-19\nLibrerías usadas  Bokeh Pandas  ","description":"Graficos sobre COVID19 usando bokeh","id":19,"section":"posts","tags":["COVID19","Python","Visualization"],"title":"COVID19 + Bokeh","uri":"https://thebooort.github.io/posts/covid_bokeh/"},{"content":"Hey reader, Bart here! My name is Bartolomé Ortiz, I'm a Ph.D. student of Computer Science at Granada University (Spain). My background is Applied Mathematics (BSc in Maths, MSc in Maths and Physics). I'm currently researching Recommender Systems for complex objects, specifically with applications in nutrition, diets, and microbiome. I work as a predoctoral researcher in a European Project (Stance4health). I'm an advocate of open science and open software. In my free time, I love to talk about math, data science, computer science, skepticism and biology. I love to give talks about those things. I'm the co-host of a science podcast: The Fluxions. I also love trail running, hiking and martial arts. ","description":"There is not such thing as boring mathematics","id":24,"section":"","tags":null,"title":"About","uri":"https://thebooort.github.io/about/"}]