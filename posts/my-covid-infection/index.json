[{"content":"aaa\n","description":"Conferencias y talleres para Asociacion Granadina de Altas capacidades","id":6,"section":"showcase","tags":null,"title":"Conferencias y talleres para Asociacion Granadina de Altas capacidades","uri":"https://thebooort.github.io/showcase/comunicaci%C3%B3n-cient%C3%ADfica/asgran/"},{"content":"aaa\n","description":"Talleres en la Feria de la ciencia del Parque de las Ciencia (Granada)","id":9,"section":"showcase","tags":null,"title":"Talleres en la Feria de la ciencia del Parque de las Ciencia (Granada)","uri":"https://thebooort.github.io/showcase/comunicaci%C3%B3n-cient%C3%ADfica/feria_de_la_ciencia/"},{"content":"Most Popular Joseki in 2021 Motivation I was just surfing on the web when I came across this data set. I add extra labels and think about the best way to show it.\nAt first I thought about making a big goban and divide it, adding the diagrams to every corner. But generating diagrams is kind of a pain in the ass, and more than 4 probably wont let the reader understand anything, so I decided to go a little bit more abstract, and use only the diagrams when hovering.\naaaaan that\u0026rsquo;s it. Some diagram need to be filled, but half of them are already there for you to look at.\nResults: Interactive!  LINKS  Most Popular Joseki in 2021- Online Go forum Flourish template Online Go: Used for the diagrams  ","description":"A collection/visualization of the 20 Most Popular Josekis played in 2021 from KifuDepot/Go4Go","id":15,"section":"posts","tags":["Visualization","Data","Flourish"],"title":"Most Popular Joseki in 2021","uri":"https://thebooort.github.io/posts/joseki2021/"},{"content":"My COVID experience En enero de 2022 tuve la mala suerte de pillar la COVID-19.\nDurante los días que mostré sçintomas, me dediqué apuntarlos y medirlos.\nEste es el resultado.\nVIZ   LINKS  No data provided this time, sorry!  ","description":"(ES) A quick chart that summarizes my experience with covid-19","id":16,"section":"posts","tags":["Visualization","Data","Python"],"title":"My COVID experience","uri":"https://thebooort.github.io/posts/my-covid-infection/"},{"content":"Coffee 2021 Wrapped   Motivation Antes de empezar:\n hay un poco de 2020 tmb XD Datos de cafés comprados, no están aquellos que he probado en cafeterías. No busco un perfil determinado al comprar café, me gusta probar todo No juzguéis duramente el hilo, que es por las risas. Café para filtro.  Toda la info proviene de cafés de especialidad\nVisualization Origen ¿De dónde vienen los cafés que he tomado este año?\n  Continentes   Mapa global   Notas de cata ¿ Y las notas de cata? ¿A qué suele saber mi café? (aunque saborear el café es mucho más que eso)\n  Procesos ¿Qué procesos ha seguido el café en su recolección?\n  Tostadores No solo es importante la recolección, el tostado es clave. ¿En qué tostadores he confiado este año? Nº de cafés diferentes, puede NO haber correlación con cantidad (paquetes de 100gr vs paquetes de 250gr)\n  Variedades ¿Qué variedades he podido probar este año? (Contadas como diferentes si vienen mezcladas con otras )\n(+ info sobre variedades: )\n  Altitud ¿En qué altitudes se encuentran las fincas? (metros sobre el nivel del mar)\nPor qué es importante \n  LINKS  Repository (code and examples)  ","description":"Almost every coffee I have drank in 2021 (and a bit of 2020)","id":17,"section":"posts","tags":["Visualization","Data","Python"],"title":"My coffee Journey... Wrapped 2021","uri":"https://thebooort.github.io/posts/coffee2021/"},{"content":"photo credits: Photo by Joe Woods on Unsplash\nVisualization of the European Go Championship\u0026rsquo;s Finals   Motivation Just a few days ago the world chess championship ended. Of course, it is a very important event that always appears in the media. FivethirtyEigth is no exception. I am not a big fan of chess (I practiced it in my high school days, but I never spent enough time to be competent at it), but like any other sporting event, I find it fascinating to keep up to date with what is going on. The FTE coverage was very interesting and in particular I found the viewing of the games very engaging. In it, they visualized the stockfish AI\u0026rsquo;s evaluation of the game.\nJust at that time, something that was happening that perhaps did not receive so much publicity, was the European Go championship. It\u0026rsquo;s been a couple of years now (covid world phase) that I don\u0026rsquo;t play Go actively, but it is still a game that I love. And precisely for this reason I thought of repeating the visualization of the good people of FTE but in the Go championship. A little to practice visualization (I considered it an interesting challenge) and another because offering that analysis could make someone understand the games a little better.\nAnalyzing Games To analyze the games I used KataGo an AI with some interesting extras. As of January 2021, KataGo is one of the strongest open source Go bots available online. KataGo was trained using an AlphaZero-like process with many enhancements and improvements, and is capable of reaching top levels rapidly and entirely from scratch with no outside data, improving only via self-play. Normally I used katago as a powerful AI to play against the machine. However, it can also be used to analyze games, and it is something that is being incorporated in the websites specialized in playing Go online. It is interesting to note that it is very likely that for the average player the variations offered by KataGo are not entirely recommendable, since a good review requires understanding what is being done (the direction and sense of play). Even so, the KataGo analysis allows to evaluate both the probability of victory and the difference in the estimated score of both players.\nTo launch this analysis, you can use the raw version of Katago directly from the terminal. But as I had in mind to analyze several games, I found this repo: analyze-sfg \nanalyze-sfg allows you to launch the katago analysis mode for several games. This mode returns a sgf file (the standard for storing go games) in which the different probabilities of each player and the variations suggested by KataGo are commented. With this I had everything to generate the graphs.\nHow to read SGF files The next step was to find (or so I wished) a pyhton library that would allow me to work with sfg files. Luckily for me there was something ideal: https://mjw.woodcraft.me.uk/sgfmill/\nSGFMill was more than I needed for the job, so by testing which nodes and which properties would lead me to the information I needed, I got a list with the evolution of the game (odds and score) and information about players, title etc.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46  import re from sgfmill import sgf with open(\u0026#34;roln90_vs_artem92_04_12-analyzed.sgf\u0026#34;, \u0026#34;rb\u0026#34;) as f: game = sgf.Sgf_game.from_bytes(f.read()) winner = game.get_winner() board_size = game.get_size() root_node = game.get_root() b_player = root_node.get(\u0026#34;PB\u0026#34;) date = root_node.get(\u0026#34;DT\u0026#34;) w_player = root_node.get(\u0026#34;PW\u0026#34;) analsyiss_comments = root_node.get(\u0026#34;C\u0026#34;) mov_list =[node.get_move() for node in game.get_main_sequence()] comment_list=[node.get(\u0026#34;C\u0026#34;) for node in game.get_main_sequence()] for node in game.get_main_sequence(): movimiento = node.get_move() # print(movimiento) comment = node.get(\u0026#34;C\u0026#34;) data_percentage=[] for s in comment_list: info = s.split(\u0026#39;*\u0026#39;) if s[0]!=\u0026#39;#\u0026#39; and len(info)\u0026gt;1: if re.search(r\u0026#39; Win rate: (.*?) \u0026#39;, info[1]).group(1) == \u0026#39;W\u0026#39;: win_rate = re.search(r\u0026#39; Win rate: W (.*?)%\u0026#39;, info[1]).group(1) number = float(win_rate) data_percentage.append(number) if re.search(r\u0026#39; Win rate: (.*?) \u0026#39;, info[1]).group(1) == \u0026#39;B\u0026#39;: win_rate = re.search(r\u0026#39; Win rate: B (.*?)%\u0026#39;, info[1]).group(1) number = float(win_rate) data_percentage.append(100-number) data_percentage_score=[] for s in comment_list: info = s.split(\u0026#39;*\u0026#39;) if s[0]!=\u0026#39;#\u0026#39; and len(info)\u0026gt;1: if re.search(r\u0026#39; Score lead: (.*?) \u0026#39;, info[2]).group(1) == \u0026#39;W\u0026#39;: win_rate = re.search(r\u0026#39; Score lead: W (.*?)\\n\u0026#39;, info[2]).group(1) number = float(win_rate) data_percentage_score.append(number) if re.search(r\u0026#39; Score lead: (.*?) \u0026#39;, info[2]).group(1) == \u0026#39;B\u0026#39;: win_rate = re.search(r\u0026#39; Score lead: B (.*?)\\n\u0026#39;, info[2]).group(1) number = float(win_rate) data_percentage_score.append(-number)   The results (Divided by game) First I try with the Quarterfinals game:\n  Quarterfinals   Meanwhile 1st and 2nd matches of the final were played:\n  Finals's 1 game     Final's 2 game   The complete infography And then, I mixed both of them and make a final full infography of the whole Final:\n  Both games   LINKS I need to update this, sorry!\n Repository (code and examples)  ","description":"Visualization of the European Go Championship's Finals","id":18,"section":"posts","tags":["Visualization","Data","Python","Artifical Intelligence","Math"],"title":"European Go Championship 2021","uri":"https://thebooort.github.io/posts/go_chart_2021/"},{"content":"The global map of math (not really, but it is close)   Motivation Lately I have been very obsessed with finding a way to create a concept map of modern mathematics. And it is not being easy. If we assumed that it was possible and that we had all the information, we would have a huge amount of information. The ramifications of mathematics are enormous and there are so many interconnected areas. It\u0026rsquo;s a daunting task and I don\u0026rsquo;t know if it\u0026rsquo;s impossible.\nBut whatever, the important thing is to have fun and learn along the way.\nI started to look for some attempts in this line. But I didn\u0026rsquo;t like any of them. It is certain that there are really interesting schemes, but the structure towards impossible a visualization like the one that was looking for.\nFinding MSC I decided, considering the size of the project, that establishing all the connections would take time, but maybe a good place to start was to find a place where everything that needed to be related was. The goal was, therefore, to find an acceptable and used classification of the branches of mathematical knowledge that exist to date.\nAnd immediately afterwards I thought of Arxiv. I started looking for how to classify the articles, what structure it had, and that\u0026rsquo;s when I got the first pleasant surprise: I found the MSC or the Mathematical Subject Classification.\nTons of data (From wikipedia) The Mathematics Subject Classification (MSC) is an alphanumerical classification scheme collaboratively produced by staff of, and based on the coverage of, the two major mathematical reviewing databases, Mathematical Reviews and Zentralblatt MATH. The MSC is used by many mathematics journals, which ask authors of research papers and expository articles to list subject codes from the Mathematics Subject Classification in their papers.\nIt basically have three levels of structure:\n  At the top level, 64 mathematical disciplines are labeled with a unique two-digit number. In addition to the typical areas of mathematical research, there are top-level categories for \u0026ldquo;History and Biography\u0026rdquo;, \u0026ldquo;Mathematics Education\u0026rdquo;, and for the overlap with different sciences\n  The second-level codes are a single letter from the Latin alphabet. These represent specific areas covered by the first-level discipline.\n  Third-level codes are the most specific, usually corresponding to a specific kind of mathematical object or a well-known problem or research area.\n  TL;DR we have about 6000 thousand tags.\nThe truth is that I didn\u0026rsquo;t expect so many. As I said before, while this classification is interesting, it is not exactly what I was looking for. But it\u0026rsquo;s a start. In the world of research, the MSC offers a classification of the areas that have the most presence and body, from which I intuit that later I can obtain information about possible connections. But wtf I hav eso much data to plot LOL.\nHOW?! I was looking for a representation that was visually interesting, but also something from which I could draw some knowledge. Having so much data, it was not going to be easy.\nFollowing the spirit of the MSC (areas of knowledge that have more specific areas) I thought that a representation as a phylogenetic tree or radial phylogenetic would be the most convenient.\nI also think that a representation with graphs and connections could be very interesting, but, without the information of the extra connections between areas, it was going to remain a little bland.\nEven so, given the volume of data, generating something readable was going to be complicated. As I also wanted it to be available in my blog, I thought of something interactive with D3.js. And that\u0026rsquo;s where my first attempt came from:\n  A interesting thought-provoking plot\u0026hellip; sure, and completely unreadable.\nAt this stage it was clear to me that in order to obtain knowledge of my graphic I would need to have basic tools. That you could navigate through it, increase it, decrease it, decide what to look for and where.\nSo, D3.js had been a very good choice from the beginning, and I found a code that was just right for me. Wherever you are Robsmuecker, KUDOS my friend. This was just what I was looking for (given that I have little knowledge about D3.js).\nI finally had an interesting and interactive presentation of math knowledge (more or less). And know, lovely reader, you have it too:\nThe final (only for now) map of mathematics Here you have a quick screenshot of the plot:\n  But, of course, I strongly encourage you to go and experiment and play with the plot by yourself.\nYou can find the final interactive map here (you can drag, click, zoom everywhere) Mathematics Subject Classification 2020 edition\nFinal thoughts It was an interesting weekend project. I learnt a lot about math classification, and at the end I have a nice diagram in my blog that everybody can use.\nHowever, it is not what I initially planned for. So I feel that it is kind of unfinished. I was thinking about get all the different maps I found in other post. And after that maybe came back and figure out the ULTIMATE DEFINITIVE SUPER math map. Yeah, that would be nice.\nLINKS  Repository (code and examples)  ","description":"Interactive plot of the CMS2020 math classification in the search of a extensive mind map of math","id":19,"section":"posts","tags":["Visualization","Data","Python","Math"],"title":"The global map of math (well, kind of)","uri":"https://thebooort.github.io/posts/mscplot/"},{"content":"Image credit: Li-An Lim\nImage credit: Markus Spiske\nLa crisis climática en Granada   Motivación Este post recoge un hilo que hice para twitter. En el explico aspectos básicos de qué se utiliza para predecir cómo nos va a afectar la crisis climática.\nAdemás, como muchas veces es algo de lo que se habla muy en global, intento ofrecer una perspectiva local, hablando de lo que esas predicciones nos dicen para la región de Granada.\nEl resultado, no es bueno, pero ofrece el claro testimonio de que si se cambia de rumbo, las consecuencias se pueden paliar.\nOs transcribo el hilo: Parece que la crisis climática nos pilla muy lejos. Pero nada de eso. Aquí van unas gráficas de las predicciones de algunos efectos que se pueden hacer notar en Granada.\nAntes de empezar:\nLas Sendas Representativas de Concentración o RCP, son trayectorias de concentración de gases de efecto invernadero (no emisiones). Básicamente nos indican un escenario de evolución de estos gases.\nPartiendo de esos escenarios, podemos hacer proyecciones regionalizadas de cambio climático , a partir de las proyecciones calculadas con modelos climáticos globales aplicadas a regiones. Para analizar los posibles impactos.\nEn las graficas salen 3 escenarios:\n RCP 4.5\nPico de emisiones en 2040 RCP 6.0\nPico de emisiones en 2080 RCP 8.5\n\u0026ldquo;Pa que queremos reducir na\u0026rdquo;  Los datos están disponibles en la página de la Agencia Estatal de Meteorología para el tiempo y el clima. Se ha hecho la media de los distintos modelos.\nEmpezamos con las estaciones mas frías.\nEvolución de las temperaturas en Otoño e Invierno:     Evolución de las temperaturas en Primavera y Verano:     No solo subirían las temperaturas, aumentaría la duración de las olas de calor:\nDuración olas de calor   ¿Habéis pasado calor alguna noche este verano? Tampoco tengo buenas noticias con respecto a ello:\nCantidad de noches cálidas   Vamos a pasar más calor pero, ¿y la lluvia?\nPues\u0026hellip;. Bajan los días de Lluvia y la precipitacion anual\u0026hellip;\nVariacion en dias de lluvia y Precipitacion anual      Sobre usar el termino crisis climática  Disclaimer: Este pequeño proyecto lo he hecho con el ánimo de aprender cosas.\nHay más predicciones y medidas, he resaltado esas por puro azar.\nEstoy lejos de ser experto, cualquier corrección es bienvenida.\nCreo que nada de lo comentado es perjudicial.\n","description":"Lo que las predicciones dicen sobre la crisis climática en Granada... nada bueno","id":20,"section":"posts","tags":["Visualization","Data","Python","Ecology","Climate Crisis","crisis climática"],"title":"¿Cómo nos afectará la crisis climática? Una perspectiva local en Granada","uri":"https://thebooort.github.io/posts/calentamiento_granada/"},{"content":"Image credit: Ivan Aleksic @ivalex Unsplash\nEste post es una actualización de uno más antiguo, dónde podréis encontrar información sobre dónde encontré los datos, tips para dibujar el gráfico y qué significa ¿Cómo afecta el confinamiento a los niveles de polución? Durante la cuarentena que estamos viviendo la reducción de la movilidad de las personas es evidente.\nLigada a esta disminución del tránsito de vehiculos particulares, vehiculos de transporte público y algunas maquinarias pertenecientes a servicios que se encuentran inactivos, muchos medios se han hecho eco de la disminución en los niveles de polución ambiental. Es más, al ser COVID una enfermedad que afecta a las vías respiratorias, ya hay algunos análisis que relacionan la polución del aire con tasas de moratilidad mas altas por COVID.\nPor este motivo me pareció interesante estudiar el caso de la ciudad en la que resido: Granada(España).\nPlot  ","description":"Evolución de los niveles de polución en Granada durante la cuarentena (actualización con datos de agosto)","id":21,"section":"posts","tags":["Visualization","Data","COVID19","Python","Pollution","Ecology","Air Quality"],"title":"El aire que respiramos durante el confinamiento (actualización con datos de agosto)","uri":"https://thebooort.github.io/posts/pollution_levels_update_agosto/"},{"content":"photo credits: Photo by Joe Woods on Unsplash\nPlotting winning probabilities in a 19x19 Go / Baduk game From this older post:\n This morning I found this post on Reddit about winning probability for black for all starting positions.\nI found this visualization quite interested but i couldn\u0026rsquo;t find any link to a source code to generate it or the data used to generate it, so I decided to replicate it myself.\n Data This time I run last version of Leela Zero v.0.17 with Chinese counting and 7.5 komi (using my GPU). I use the gtp engine via Lizzie an try every strating position.\nI will upload the data via sfg, as Lizzie let you safe prob\u0026rsquo;s data. If I haven\u0026rsquo;t uploaded it yet and want to use the data, just send me a message.\nPlotting considerations Refer to this older post in order to know a little bit more about the code.\nResults Again, I am quite happy with the results. I think these heatmaps looks better than just adding squares to every point. It is just a matter of taste! However, I don\u0026rsquo;t LOVE the colormap. It\u0026rsquo;s close to the initial style, but the percentage variations are too extreme and I haven\u0026rsquo;t found a colormap that, in my opinion, reflects well the data (aesthetically). I know I could design one by hand to be better, but, for now, I think jet does its job.\n  with colorbar     Explicit probabilities   ","description":"Update! Heatmap of probability for black to win for all starting positions in a 19x19 board (Leela Zero GPU V0.17)","id":22,"section":"posts","tags":["Visualization","Data","Python","Machine Learning","Go","Baduk"],"title":"Winning probabilities in Go 19x19 (LeelaZero)","uri":"https://thebooort.github.io/posts/baduk-go-winning-probabilities-2/"},{"content":"The Plot Classifier   (Yeah I designed the logo too! Does it count as artistic design knowledge? :P) Repository All this info and more can be found at the repo\nAny serious aspect you might find in this repo is purely coincidental Hey! the tool you don\u0026rsquo;t even know you need!\n(Silly summer project to know the name of that cool plot/chart/visualization you saw on a webpage)\nIndex  Motivation Approach  Data Collect ML Approach  Neural Net architecture Results Nets trained you can find here   Streamlit  Screenshots   Heroku    Motivation This project was born from a personal doubt that I have come across several times: What is the name of that cool graphic?\nAnd I\u0026rsquo;ve always lost some time in finding the exact name or the way to call it of some bookstores. It is true that I talk about more complex graphics, but I decided that it could be interesting to start with the basics and perhaps at some point the tool can help some beginner to discover their first graphics.\nIn addition, the main reason for this project was to try and play with the possibilities offered by streamlit as a way of providing a preliminary result in machine learning. In that aspect I think it has been a success, since I have acquired what is necessary to make an attractive and functional streamlit page with a model that I have implemented.\nApproach Data collect I didn\u0026rsquo;t find anything usable on the internet. So I thought of two alternatives, one to generate my own graphics and the other to get them from some search engine.\nGenerating my own graphics seemed very interesting and maybe in the future I will feed the system with randomly generated graphics, however I think that the investment of time would not be rewarded, as the options are endless.\nOn the contrary, I made the decision to get a group of images from the internet, using the search engines. Obiously the quality of the images (and I don\u0026rsquo;t mean their quality in pixels) would decrease as the amount of images to extract from a single query search increases. I reached a middle point where most of the extracted images match the query text.\nFinally I reviewed the 14 groups of graphics selected in this initial version of the application and eliminated some of the erroneous images I found.\nThe final data set was not very large. During the pre-processing I tried the data augmentation, but the difference in quantity was not going to be very big. The solution for this is explained in the next section.\nML approach Although machine learning is essential for this project, as I explained in the motivations section, it was not the final goal. Therefore I had to be careful with the development of it, to balance effort and result.\nI decided to use Keras for this purpose. I have interest in other libraries but Keras was the one I used the most, this guaranteed familiarity with the process.\nDespite this, generating a good image multi-sorting engine is not an easy task. Even less having so many categories (with many possible options) and so little data for training.\nI decided as an intermediate solution to use a pre-trained network and perform transfer learning. That is, take a pre-trained neural network and add some final layers that classify according to my categories.\nTo choose the neural network to use, I simply looked for the one that was giving better results and that was not a bestiality. I found MObilnet from Google and it was just what I was looking for: \u0026ldquo;Designed to effectively maximize accuracy while being mindful of the restricted resources for an on-device or embedded application. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used\u0026rdquo;.\nThe network weighed very little and I could retrain it very easily, to get good results with minimal effort.\nNN architecture You can check the neural net arcxhitecture here\nResults First the model loss:\n  And the accuracy reached.\n  Nets Trained In the project repo I have 3 .h5 archives with weights for you to experiment with:\n keras_model.h5: Trained with google services with no curated dataset (used at the very beginning) per_trained.h5: Trained with transfer learning, data augmentation, all images curated, and all images used(no validation). per_trainedv2.h5: Same of above, but with validation images.  Streamlit dev Screenshot When you enter the app you a see a brief description and a menu to upload the photo\n  After that, the photo is classified (with a loading menu) an then it tells you the name, some other examples and some links to coding libraries that make it .\n  Heroku deployment I finally ended deploying this app with Heroku. I used another repository for that task and follow this tutorial for that. Here you can play with the app and its limits. I hope that in some near future I may have time to upgrade and improve, but in the meantime it was a funny and educative project.\nHere you have! (It takes a little bit to load!)\nTHE PLOT CLASSIFIER TOOL ","description":"(Summer project) Identifiying chart/plot name from images with deep learning ","id":23,"section":"posts","tags":["Visualization","Data","Python","Machine Learning","Deep Learning","Transfer learning"],"title":"The PLot Classifier","uri":"https://thebooort.github.io/posts/theplotclassifier/"},{"content":"OEIS Sequences Visualized   Motivation There is not much to tell here. If you don\u0026rsquo;t know what the OEIS is, there are hours of fun waiting for you. It is basically an online encyclopedia that collects almost all relevant integer sequences. You can find everything, but in particular many properties and curious numbers.\nLooking at some pictures of number visualizations I discovered one that assigned a color to each of the 10 digits and colored a grid according to the order of the digits of the chosen constant. I found it for $\\pi$ but immediately I connected this visualization with the OEIS. I liked the idea of a visual identity card for the sequences of numbers and I jumped into it.\nIn order to encourage a little bit the project (and test my script), I asked my followers in twitter to interact with a tweet and I would assign them a sequence and its photo. I have many interactions!\nA fun mini project.\nResults Check the twitter thread with all the photos HERE\nOtherwise, here you have some images:\n      LINKS  Repository (code and examples)  ","description":"Code for visualize OEIS sequences in a cool way + twitter interactions","id":24,"section":"posts","tags":["Visualization","Data","Python","Math"],"title":"Visualizing OEIS sequences","uri":"https://thebooort.github.io/posts/oeisviz/"},{"content":"  20 años y 20 horas Quiero invitarte a reflexionar: ¿dónde estabas hace 20 años?\nEchar la vista atrás tanto tiempo puede ser complicado, pero es necesario para el tema que queremos tratar hoy. En un periodo tan largo caben muchas experiencias, tanto buenas como malas. Incluso, dependiendo del lector, ese período puede que abarque toda su existencia.\nY antes de que empieces a sentir vértigo por lo rápido que pasa la vida y lo viejo/a que te encuentras, quiero que te quedes con esa visión de la enorme cantidad de tiempo que suponen 20 años, 175.200 horas.\n20 años fueron los que Charles Eugène Delaunay (1816-1872), matemático francés, necesitó para calcular la órbita de la luna, intentando mejorar los intentos de sus predecesores, Laplace y Le Verrier.\nEste periodo se dividió en dos etapas de 10 años, una para hacer las cuentas y otra para verificarlas. En 1847, con la públicación de su metodología en la revista de la Academia francesa de París, se daba el pistoletazo de salida a una prueba calculística que iba a requerrir mucho de Delaunay.\nLa primera parte del rompecabezas para calcular la órbita de la luna fue una aproximación. Delaunay consideró las órbitas de la luna y la tierra eran elípticas y se encontraban en planos diferentes. Posteriormente añadió las perturbaciones producidas por el Sol y otros planetas, hasta obtener un sistema Hamiltoniano (un sistema de ecuaciones diferenciales que describe el movimiento de la luna). El primer paso estaba dado.\nTener el sistema y poder usarlo o resolverlo eran cosas muy distintas. La única opción que le quedó a Delanay (y que sigue siendo utilizada en muchas ocasiones hoy en día) fue traducir su sistema diferencial a una expresión en suma de potencias.\nSiendo parcos en palabras, Delaunay pasó de resolver un sistema de ecuaciones a calcular una suma de muchisimos términos. ¿Dónde está el truco? Los coeficientes de esos términos debían ser obtenidos a mano mediante transformaciones algebraicas, y cuánta más exactitud quería, más términos debía poner (y por tanto, más coeficientes y más cáclulos a mano). Llegó hasta orden 7, lo que se traduce en 478 términos, los cuales tenían por coeficientes polinomios que también debían ser calculados.\nEn 1867, Charles publica su segundo tomo de resultados, concluyendo su periplo celeste. Sin embargo, los resultados, aunque acertados, no tenían la precisión que Delaunay esperaba, con márgenes de error de hasta 100km. ¿Fue un esfuerzo en bano? ¿Se había equivocado Delaunay?\nTardaríamos casi 100 años en saber que pasó.\nEl problema de encontrar el fallo en el trabajo de Delaunay era cuantitativo. A Delaunay le había llevado unos 10 años comprobar todos sus cálculos. Sin la perspectiva de que encontrar el error mejorase sus resultados enormente, nadie iba perder 10 años de su vida sin garantías.\nAdemás, hubo más intentos tras Delaunay. Intentos que mejoraron sus resultados y avanzaron nuestra comprensión de la mecanica celeste y la luna.\nAsi que para resolver el misterio hubo que esperar a una de las grandes revoluciones del siglo XX: el algebra computacional y el cálculo simbólico (vamos, que le cargamos el muerto a un ordenador, que no se queja).\nHasta 1960 los ordenadores que existían se llevaban bastante bien con las cuentas. Sumas, restas, matrices\u0026hellip; lo que en matemática aplicada conocemos como cálculo numérico. Dentro de este área, podemos atacar el problema de la órbita lunar a traves de la integración numérica (usando las funciones de nuestro sistema y operaciones básicas). Desgraciadamente las aproximaciones analíticas (como la de Delaunay) que requieren transformar nuestro sistema con manipulaciones algebraicas complejas, y aunque nos dan una mejor comprensión de lo que está pasando, no eran programables en un ordenador.\nLo que nos permite el algebra computacional es justo eso, manipular las expresiones, trabajar con ellas, con los símbolos de la matemática. Y gracias a ello resolvimos el misterio de Delaunay.\nAndré Deprit (1926-2006), fue uno de esos matemáticos pioneros que consiguió que el ordenador hablase el lenguaje matemático. Junto con J.M.A. Danby y Arnold Rom desarrollan un paquete matemático: MAO - Mechanized Algebraic Operations. Este paquete fue un precursor en su clase, potenciando el desarrollo de muchos otros lenguajes de cálculo simbólico (como Macsyma, el abuelo de wxMaxima o Mathematica, sin el que muchos estudiantes no pueden vivir hoy en día). Para mostrar las bondades de su trabajo, Deprit y su equipo debían escoger un problema interesante.\nGracias a la transformación explícita basada en series de Lie, Deprit y sus colaboradores pudieron replicar todas las transformaciones algebraicas del trabajo de Delaunay, consiguieron que el ordenador hablase nuestro mismo lenguaje, con variables, coeficientes y cálculos itnermedios. Y todo ello, para descubrir que casi al final del sumatorio, Delanay había confundido un 33/16 por un 23/16. Un fallo lo tiene cualquiera.\n20 horas de computación bastaron para encontrar un error en un trabajo de 20 años.\n20 horas que sirvieron para mostrar la potencia de los nuevos lenguajes de cálculo simbólico, y la importancia que iban a tener en los años venideros.\nNotas Delaunay no sólo trabajó en mecánica celeste. Su trabajo como matemático se ve reflejado en otras áreas. Muestra de ello es un artículo publicado en 1841, donde demuestra que las únicas superficies de revolución con curvatura media constante, son las superficies obtenidas por rotación de las ruletas cónica. Estas superficies se denominan actualmente, en su honor, superficies de Delaunay.\nBibliografía  La columna de la Academia (La Verdad, 9 de abril de 2016) https://www.um.es/geometria/files/Delaunay.pdf Tony Hurlimann. 1999. Mathematical Modeling and Optimization: An Essay for the Design of Computer-Based Modeling Tools. Kluwer Academic Publishers, USA. Introduction à la théorie de la Lune de Delaunay https://journals.openedition.org/bibnum/683 H S Dumas, K R Meyer, D S Schmidt. 1995. Hamiltonian Dynamical Systems: History, Theory, and Applications. The IMA Volumes in Mathematics and its Applications, Springer. Deprit A, Henrard J, Rom A. Lunar Ephemeris: Delaunay\u0026rsquo;s Theory Revisited. Science. 1970 Jun 26;168(3939):1569-70. Nota Necrologica: Prof. Dr. Andre Deprit http://www.raczar.es/webracz/ImageServlet?mod=publicaciones\u0026amp;subMod=revistas\u0026amp;car=revista61\u0026amp;archivo=p181.pdf  Images credit:   Cabecera: Joseph D.M. White\u0026rsquo;s Youtube channel https://www.youtube.com/watch?v=ugxRx7alXb4\u0026amp;feature=emb_logo\n  Photo by Neven Krcmarek on Unsplash: https://unsplash.com/photos/9dTg44Qhx1Q\n  ","description":"Sobre la luna, Delaunay , Deprit y el inicio del álgebra computacional","id":25,"section":"posts","tags":["Comunicación","Comunicación Científica","Ciencia","Divulgación","Matemáticas","Algebra","Computación","Computación Matemática"],"title":"20 años y 20 horas","uri":"https://thebooort.github.io/posts/moon_maxima/"},{"content":"  Esta semilla parece una mierda La vida está llena de trampas, de engaños. No importa que tus intenciones sean buenas, o que te creas una persona muy avispada (Drunning effect warning), en algún momento te la han colado. Cada día, la sobrecarga de información nos dificulta tomar decisiones y formarnos una opinión, dejándonos a merced de que nuestro espíritu crítico decida plantar batalla y nos obligue a investigar sobre esa foto incendiaria que nos ha llegado por WhatsApp.\nSi ya es difícil para nosotros, imagináos para un escarabajo.\nEn particular hablamos del simpático Epirinus flagellatus (Fabricius,1775) habitante de las zonas de Sudáfrica. Este escarabajo pertenece a la familia Scarabaeoidea, es decir, los comúnmente llamados escarabajos peloteros. El ciclo de vida de estos simpáticos insectos está estrechamente relacionado con las heces. Casi como expertos moldeadores, las especies de la familia Scarabaeoidea desgajan partes de estiércol dejado por otros animales y lo moldean hasta obtener una cuasi-esfera. Valiéndose de sus patas traseras, transportan la \u0026ldquo;pelota\u0026rdquo; hasta el lugar donde han excavado su nido, y allí depositan todas las que pueden.\nLa siguiente pregunta que mucha gente se hace al ver a estos escarabajos rodar esferas de caca (podemos llamarlas hecesferoides, por favor(?)) es qué motivo lleva a un animal a hacer esto. La respuesta, cómo muchas veces en la naturaleza, son los niños, o más bien, la descendencia. En estas bolas de heces se insertarán más adelante los huevos del escarabajo, que crecerán dentro de ellas alimentándose bien calentitos (debido a la fermentación del estiércol).\nA simple vista, estos escarabajos llevan una vida sencilla, y parecería hasta raro que algún otro animal o planta quiera intentar engañar a un insecto que pasa su vida comiendo o transportando caca. Nada más lejos de la realidad. Extrayendo la parte escatológica del proceso, lo cierto es que la vida adulta de estos escarabajos se puede resumir en trasladar y enterrar. Y es algo que hacen bastante bien. ¿Y quiénes se pueden beneficiar al máximo de esta particular actividad? Las plantas.\nAsi que imaginaos a nuestro escarabajo, transportando una bola que se parece mucho a una pelota de caca y que desde luego huele como tal. Dejándola orgullosamente en su nido. Y todo para acabar descubriendo que lo que transporta, no sólo no son heces, si no que además está demasiado duro para ser útil de cualquier forma: está transportando una semilla.\n  Ceratocaryum argenteum   El Espirinus flagelatus ha sido engañado. La culpable, Ceratocaryum argenteum, una planta herbácea de la familia Restionaceae.\nTodas las plantas buscan expandirse y difundir al máximo sus semillas por el medio. De hecho, las plantas poseen una gran cantidad de técnicas con este fin, tantas, que su estudio tiene un área dentro de la botánica: Dispersión de los propágulos.\nFórmulas de dispersión hay muchas, sin embargo nos interesan aquellas que pertenecen a la zoocoria, aquella en la que el agente que realiza el transporte de las semillas es un animal. A este apartado pertenece nuestra culpable, la C.argenteum, que produce una particular semilla de tamaño similar a las heces de antílope. Esta semilla, para completar el disfraz, emite volátiles que se han encontrado en las deposiciones de estos herbívoros (se decir, \u0026ldquo;huelen\u0026rdquo; muy similar). Ante tal intrincado ardiz, el E.flagellatus no puede más que caer en la trampa.\n  a.) y b.) muestran una semilla de C.argentum. f.) Espécimen de E.flagellatus. g.) Heces de antílope   Es conveniente resaltar que el escarabajo no consigue nada de este transporte, no hay una relación de mutuo beneficio. La C.argentum se vale de una exquesita técnica de biomímesis (o biomimética) afilada a la luz de la evolución. Este particular recurso se describió por primera vez en 2015 y es único en sus características. De hecho, la dispersión de estas semillas era un misterio, habitualmente atribuida a los roedores, pero sin pruebas que lo sustentase.\nAl final, tuvieron que llegar los escarabajos, expertos en la materia (fecal), para mostrarnos que al igual que no es oro todo lo que reluce, no es caca todo lo que rueda.\nBibliografía   Jingshang CHE,Hailong SUN,Chenjie XIAO, et al. Why information overload damages decisions? An explanation based on limited cognitive resources[J]. Advances in Psychological Science, 2019, 27(10): 1758-1768.\n  Soroush Vosoughi, Deb Roy, Sinan Aral. The spread of true and false news online. Science, 09 Mar 2018 : 1146-1151\n  Burger BBV. First Investigation of the Semiochemistry of South African Dung Beetle Species. In: Mucignat-Caretta C, editor. Neurobiology of Chemical Communication. Boca Raton (FL): CRC Press/Taylor \u0026amp; Francis; 2014. Chapter 3. Available from: https://www.ncbi.nlm.nih.gov/books/NBK200987/\n  Midgley, J. J., White, J. D., Johnson, S. D., \u0026amp; Bronner, G. N. (2015). Faecal mimicry by seeds ensures dispersal by dung beetles. Nature Plants, 1: 1-3.\n  BYRNE, MARCUS, and HELEN LUNN. Dance of the Dung Beetles: Their Role in Our Changing World. Wits University Press, 2019. JSTOR, www.jstor.org/stable/10.18772/12019042347.\n  Post donde descubrí la noticia: https://www.kew.org/read-and-watch/deceiving-seed-dispersal\n  Images credit:  Cabecera: Joseph D.M. White\u0026rsquo;s Youtube channel https://www.youtube.com/watch?v=ugxRx7alXb4\u0026amp;feature=emb_logo Foto de la C.argenteum Gif y foto del estudio sobre el E.flagellatus: Midgley, J. J., White, J. D., Johnson, S. D., \u0026amp; Bronner, G. N. (2015). Faecal mimicry by seeds ensures dispersal by dung beetles. Nature Plants, 1: 1-3.  ","description":"Sobre escarabajos cegatos y plantas malévolas","id":26,"section":"posts","tags":["Comunicación","Comunicación Científica","Ciencia","Divulgación","Biología"],"title":"Esta semilla parece una mierda","uri":"https://thebooort.github.io/posts/poop_and_seeds/"},{"content":"Image credit: Asociación española de Comunicación científica\nCienciaEnRedes2020 Las palabras importan: Leonor Parcero La primera charla que destaco es la ofrecida por Leonor Parcero López de la Universidad de Vido . De esta charla he sacado unas cuántas ideas que creo que son interesantes. Algunas no son nuevas, y ya había reflexionado sobre ellas antes, pero conviene tenerlas en cuenta y volver a ellas siempre que sea necesario.\nEn primer lugar y como reflexión general, debemos tener en cuenta qué hacemos y cómo llamamos a lo que hacemos. El problema de la notación no es ajeno a las matemáticas, la buena definición de cuál es nuestra actividad constituye el primer paso para entenderla adecuadamente, pero también para planificarla, sacar el máximo rendimiento y, en un fin último, acreditarla en aquellos procedimientos que lo requieran. Habréis notado que he omitido la palabra coumincación científica (o similares). El motivo es sencillo: creo que esa reflexión es transversal y útil para muchisimas áreas (mas generales o más específicas).\nEn segundo lugar destaco la notación que Leonor describe dentro del ámbito de la comunicación. Las diferencias entre las áreas están bien expuestas, y me parece una notación muy interesante para decidir el enfoque que quieras darle a tu comunicación científica (el mapa lo traza teniendo en cuenta objetivos, canales y públicos). Reproduzco aquí el mapa conceptual y os comento brevemente la descripción que da Leonor:\n Difusión: Contar los resultados de la investigación. Su objetivo sería un público preparado y/o experto, ya sea científicos en el área o en campos similares, industria, gobierno, responsables de tomas de decisiones en instituciones científicas y medidadores (periodistas y divulgadores).\nTradicionalmente se hablaba de la misma solo en congresos cientificos, pero parece haber una ampliación de su significado. Divulgación: Contar conocimiento científico a un público lego en la materia. Con lenguaje códigos y medios compartidos con ella (de forma atractiva/entretenida). Se suele usar en su lugar cultura científica. Los objetivos pueden ser varios (participacion ciudadana, fomento vocacional, diversion, etc) Periodismo científico: Especialidad periodística centrada en contar hechos sobre ciencia y tecnología. Caben nuevas investigaciones, pero también hechos relevantes del sector. Es, por encima de todo, periodismo (con sus características). Algunos productos podrían considerarse divulgativos.  Reflexión interesante: Vale que queramos divulgar proyectos, pero, debemos preguntarnos sobre el público destinatario, sobre el mensaje, el medio, etc. Estas preguntas son clave y debemos hacerlas antes de comenzar a divulgar sobre nuestros proyectos. No hay que olvidar que no sólo es un trabajo duro consolidar una comunidad, si no también mantenerla.\nDivulgar sin aspavientos, ahora más que nunca: Marta Macho Otra de las charlas que me gustaría destacar es la de Marta Macho, investigadora de la Universidad del País Vasco. Más que analizarla, me parece una charla para disfrutarla. Estoy muy deacuerdo con muchas de las cosas que comenta Marta, es una ponente magnífica. Me quedo con algunos momentos que quizás sean espontáneos, pero su dicción y cadencia de palabras y frases me encanta. No se si será improvisado, pero su forma de transmitir la tengo en cuenta mientras encuentro la mía.\n Cuando divulgas, debes estar formado en ese tema. Debes conocer los subterfugios de la teoría para acceder a las claves y poder contestar a las preguntas que te hagan, para tender nuevos caminos a esas respuestas. Divulgar es ayudar a descubrir. No estas dando clase, si no transmitir las claves, motivar a saber más. Rigor, humildad, sin prepotencia y admitir que no lo sabes todo. El humor es un medio para emocionar con ciencia, pero el humor es en parte, cultural, por lo que tu mensaje puede no llegar a su destinatario.  Marta comenta que su manera de divulgar favorita es el directo, que permite cambiar, improvisar, adaptar el mensaje a su destinatario. Aboga por una divulgación en privado, en pequeños grupos, donde la interacción esté servida.\nExperiencias bajo sospechas y verdades relativas: Joaquín sevilla ¿Qué camino hay entre las evidencias centíficas y las decisiones sociales y/o políticas? ¿Qué relevancia social aparece en ese camino? Joaquín Sevilla de la Univesidad de Navarra propone una visualización que me ha gustado mucho basada en triángulos.\nEse camino de agregación de conocimiento, que parte de la certeza científica pero a medida que avanza se ve asaltado por aspectos sociales, culturales e imprecisiones de todo tipo, a veces es más largo o más pqueño. Cuando el camino es largo y necesitamos una integracion multidisciplinar que permea en muchos aspectos sociales como la política, es habitual que aparezcan bifucaciones (por interpretaciones culturales) que se alejan a medida que el camino avanza. Si esas interpretaciones dejan de estar cimentadas en la evidencia científica aparece la pseudociencia.\nTomar una decisión social:\n no es evidente. no es exclusivamente científico. Debe provenir de una agregación de conocimiento. Supone una incorporación de valores.  Comunicación de la ciencia, consideraciones sociales:\n Nivel de certeza buscado (posicionarse) Evaluar la complejidad del camino (cuanto más lejos, más necesario es agregar voces y posiciones) Respeto a la cultura del receptor (modelo adaptativo de la comunicacion científica) Fraude por conflictos de intereses.  Me quedo con ganas de ahondar en el modelo adaptativo de la comunicación científica, quizás os caiga post del tema.\n","description":"Algunos apuntes que he tomado de las charlas de CienciaEnRedes2020","id":27,"section":"posts","tags":["Comunicación","Comunicación Científica","LaTeX","Ciencia","Divulgación"],"title":"Algunos apuntes de CienciaEnRedes2020","uri":"https://thebooort.github.io/posts/cienciaenredes1/"},{"content":"Image credit: Ivan Aleksic @ivalex Unsplash\nEste post es una actualización de uno más antiguo, dónde podréis encontrar información sobre dónde encontré los datos, tips para dibujar el gráfico y qué significa ¿Cómo afecta el confinamiento a los niveles de polución? Durante la cuarentena que estamos viviendo la reducción de la movilidad de las personas es evidente.\nLigada a esta disminución del tránsito de vehiculos particulares, vehiculos de transporte público y algunas maquinarias pertenecientes a servicios que se encuentran inactivos, muchos medios se han hecho eco de la disminución en los niveles de polución ambiental. Es más, al ser COVID una enfermedad que afecta a las vías respiratorias, ya hay algunos análisis que relacionan la polución del aire con tasas de moratilidad mas altas por COVID.\nPor este motivo me pareció interesante estudiar el caso de la ciudad en la que resido: Granada(España).\nPlot  Conclusions! La tendencia visible ya es muy destacable. Los niveles están muy reducidos frente al año anterior. Puede que tengamos un pequeño repunte que podemos correlacionar con el fin del confinamiento más estricto, pero aún así los niveles siguen muy bajos y, desde luego, son buenas noticias.\n","description":"Evolución de los niveles de polución en Granada durante la cuarentena (actualización con datos de abril)","id":28,"section":"posts","tags":["Visualization","Data","COVID19","Python","Pollution","Ecology","Air Quality"],"title":"El aire que respiramos durante el confinamiento (actualización con datos de abril)","uri":"https://thebooort.github.io/posts/pollution_levels_update_abril/"},{"content":"  We can go for a quick run, but\u0026hellip; where? From now on (technically May 2nd), in Spain (specifically in my city Granada), we are allowed to go for a quick run or walk in the morning. Those are really good news for every runner and trail runner I know, but with great power comes great responsability.\nMy main concern about going outside for a quick run is how many people will I encounter while running. The social distance that we should keep between other dramatically increase when practicing sports, as this article of urban physiscs says:\nOn the basis of these results the scientist advises that for walking the distance of people moving in the same direction in 1 line should be at least 4–5 meter, for running and slow biking it should be 10 meters and for hard biking at least 20 meters. Also, when passing someone it is advised to already be in different lane at a considerable distance e.g. 20 meters for biking.\n  Image from: Towards aerodynamically equivalent COVID-19 1.5 m social distancing for walking and running. Blocken,B. et al (2019-Preprint)   After reading all the information, next step is to enumerate all the parameters I have to take into account when designing a gps track for run training.:\n  I think it can be quite hard to take into account the running distance (10 meters can be hard to estimate while constant movement). Furthermore if you add more people to the equation, complexity increases exponentially (well, this need to be proven, but you get the point 👅). So my main idea is to go for a run quite early in the morning or quite late in the night, trying to minimize the number of people that can be affected by my exercises (or that can affects my health).\n  Other thing to take into account is that there is not distance limit within the city, so I have the entire city to choose where to go, that is really great for me as I don\u0026rsquo;t like to run in circle multiples times. The final running track then, can be reaching one point and coming back or some kinf of circular track.\n  No more than 5-6 kilometers the first day (we need to take it easy those first days!).\n  The first point of these is the one harder to analyze:\nAs there is no distance limits,I really think there will be probably tons of people running along their old and known tracks (I was thinking about it myself, because running in a familiar route is easier and you can adjust your exercise according to how you feel/your pace/your goals, etc), so if I want to avoid them I just have to choose those streets with less \u0026ldquo;runner-traffic\u0026rdquo;.\nHow do I do that? Well well,let\u0026rsquo;s use some data science on it!\nGetting the data I made some research about public datasets available about mobility inside my city. However (big ironic surprise) there is not a single data about mobility. So I had to change my strategy.\nStrava Raw data for this task is very hard to obtain. I remembered that Strava published a global heatmap with useful information. This heatmap shows \u0026lsquo;heat\u0026rsquo; made by aggregated, public activities over the last two years, classified by sport and it is updated monthly. This address two points: been able to use the data now, and bee able to adjust my conclusions in a month if anything changes.\n  Strava map (accessed April 30)   Sadly, I only can get visual information from it. You are forced to purchase an app create by strave to get all the data, and I am not that rich (or have time to fight with bureocracy to get a deal). If any of you knows any interesting dataset related to this task, please let me know!\n  Strava map (accessed April 30)   Garmin Garming offers another similar heatmap. I first talk about Strava because, as far as I am concern, to check the Garmin heatmap you need to be registered and have a Garmin dispositive.\nThe map can be consulted at you personal account. The main difference (strictly for me ) is that Garmin allow me to mark a track while I am consulting the map, and upload this GPS track to my watch.\nFor that reason I am going to use primarly Garmin data, but in this post y also analyze Strava data-\nAnalyzing the data Strava I live near calle San Juan de Dios so I will set my start goal over there. I zoom into the map to see what I have near my spot:\n  Strava map (accessed April 30)   No surprises here, common and bigger streets are often used by other runners. Taking into account where I am going to start, I should try to minimize the most \"common\" streets, this means that Gran Via, Avenida Fuente Nueva, Calle Reyes Católicos y Camino de Ronda mostly delimit possible tracks. Small streets inside the previous demarcations are very uncommon, so I should try to wander through streets like calle Buensuceso or Calle Párraga. In addition crossing hot-points like recogidas would aloow me to make a bigger circuit while minimizing my exposure. However, I have to be carefull with crossing other common routes like Camino de Ronda, as it only leads to more common spots.\nGarmin Let\u0026rsquo;s see Garmin\u0026rsquo;s data instead. One of the things I like is the way Gramin presents the information. Data don\u0026rsquo;t blott street info and the detail level is very high.\n  Garmin map (accessed April 30)     Garmin map (accessed April 30)   About the analysis I can make about the map, it is more or less the same as in Strava. Taking into account that both resources show the same information I am confident about the data signification (although the data may overlaps, as Garmin offers the option to upload track from strava.\nFinally I think I have enough information to make an useful track for my next running sessions.\nConclusions! Taking into account the previos information I design the following track.\n  Garmin map (accessed April 30)   As it is my first running session after a long time, I limit the total distance to 5km (I introduce manually the time per kilometer, but it may change depending on my sensations, however I expect to run slowly at least the first week). Almost every meter of the track goes in \u0026ldquo;low-frequency\u0026rdquo; streets where I think I won\u0026rsquo;t encounter other runner. There are 4 main points that cross \u0026ldquo;high-frequency\u0026rdquo; places, but I think the exposure (with social distance and the schedule I am planning) will be low.\nDo you need to do all of this with your sessions? Obiously no. If know which street avoid, but it is not always easy to choose an alternative track. I like to make my decissions with some amount of data to be as objetive as possible.\nIf you have any ideas, corrections or suggestions, let me know!\nImages credit:  Photo by Flo Karr on Unsplash Photo by Chander R on Unsplash  ","description":"Analyzing Strava data to know where to run with less human contact","id":29,"section":"posts","tags":["Visualization","Data","Python","Running"],"title":"Where to run after confinement?","uri":"https://thebooort.github.io/posts/where-to-run/"},{"content":"photo credits: Photo by Joe Woods on Unsplash\nPlotting winning probabilities in a 9x9 Go / Baduk game This morning I found this post on Reddit about winning probability for black for all starting positions.\nI found this visualization quite interested but i couldn\u0026rsquo;t find any link to a source code to generate it or the data used to generate it, so I decided to replicate it myself.\nI am starting with a 9x9, because I am not familiar with any parser to exchange data with a go engine. Next step is to make all process via a python script and been able to replicate all boards.\nThis quick project take me just one hour, and I am sure everything can be automated or made better, but at the end you have the data and the code, so feel free to upgrade or change anything you want.\nData For the data I just run Leela v.0.11.0 with Chinese counting and 7.5 komi. I use the gtp engine with Sabaki an try every strating position.\nPlotting considerations I get the board from this question in stackoverflow, I simply add star points and different configurations (one for plotting with color bar and other with numbers or without them.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  # with colorbar #fig = plt.figure(figsize=[13,10.5]) # with numbers fig = plt.figure(figsize=[8,8]) fig.patch.set_facecolor((1,1,.8)) ax = fig.add_subplot(111) # draw the grid for x in range(9): ax.plot([x, x], [0,8], \u0026#39;k\u0026#39;) for y in range(9): ax.plot([0, 8], [y,y], \u0026#39;k\u0026#39;) # scale the axis area to fill the whole figure ax.set_position([0,0,1,1]) # get rid of axes and everything (the figure background will show through) ax.set_axis_off() # scale the plot area conveniently (the board is in 0,0..18,18) ax.set_xlim(-1,9) ax.set_ylim(-1,9) # draw star points s1, = ax.plot([2,6,6,2,2,4],[2,2,6,2,6,4],\u0026#39;o\u0026#39;, markersize=10, markeredgecolor=(0,0,0), markerfacecolor=\u0026#39;k\u0026#39;, markeredgewidth=2)   Also, I think data could be better but, adding points and values like array let me easily plot every point, been able to plot labels if I want and been able to add simply the colormap.\nResults I am quite happy with the aesthetic of the results. I think these heatmaps looks better than just adding squares to every point. It is just a matter of taste! At least you have all the data and the code to make similar things or your own variations. 😄S\n  with colorbar     Explicit probabilities   Todo  Exchange (with a parser) data beetween python and go engine Replicate for 13x13 and 19x19  Source code and Data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84  #!/usr/bin/env python3 # -*- coding: utf-8 -*- \u0026#34;\u0026#34;\u0026#34; Created on Sun Apr 26 \u0026#34;\u0026#34;\u0026#34; import matplotlib.pyplot as plt import numpy as np # create a 8\u0026#34; x 8\u0026#34; board # with colorbar #fig = plt.figure(figsize=[13,10.5]) # with numbers fig = plt.figure(figsize=[8,8]) fig.patch.set_facecolor((1,1,.8)) ax = fig.add_subplot(111) # draw the grid for x in range(9): ax.plot([x, x], [0,8], \u0026#39;k\u0026#39;) for y in range(9): ax.plot([0, 8], [y,y], \u0026#39;k\u0026#39;) # scale the axis area to fill the whole figure ax.set_position([0,0,1,1]) # get rid of axes and everything (the figure background will show through) ax.set_axis_off() # scale the plot area conveniently (the board is in 0,0..18,18) ax.set_xlim(-1,9) ax.set_ylim(-1,9) # draw star points s1, = ax.plot([2,6,6,2,2,4],[2,2,6,2,6,4],\u0026#39;o\u0026#39;,markersize=10, markeredgecolor=(0,0,0), markerfacecolor=\u0026#39;k\u0026#39;, markeredgewidth=2) b = np.array([[ 24.45, 29.95, 31.51, 32.48, 33.11, 32.48, 31.51, 29.95, 24.45], [ 29.95, 37.32, 40.28, 41.36, 40.8 , 41.36, 40.28, 37.32, 29.95], [ 31.51, 40.28, 44.5 , 45.46, 46.13, 45.46, 44.5 , 40.28, 31.51], [ 32.48, 41.36, 45.46, 48.79, 48.03, 48.79, 45.46, 41.36, 32.48], [ 32.48, 41.36, 45.46, 48.79, 48.03, 48.79, 45.46, 41.36, 32.48], [ 33.11, 40.8 , 46.13, 48.03, 48.03, 48.03, 46.13, 40.8 , 33.11], [ 31.51, 40.28, 44.5 , 45.46, 46.13, 45.46, 44.5 , 40.28, 31.51], [ 29.95, 37.32, 40.28, 41.36, 40.8 , 41.36, 40.28, 37.32, 29.95], [ 24.45, 29.95, 31.51, 32.48, 33.11, 32.48, 31.51, 29.95, 24.45]]) y_array = np.array([[ 0,1,2,3,4,5,6,7,8], [ 0,1,2,3,4,5,6,7,8], [ 0,1,2,3,4,5,6,7,8], [ 0,1,2,3,4,5,6,7,8], [ 0,1,2,3,4,5,6,7,8], [ 0,1,2,3,4,5,6,7,8], [ 0,1,2,3,4,5,6,7,8], [ 0,1,2,3,4,5,6,7,8], [ 0,1,2,3,4,5,6,7,8]]) x_array = np.array([[ 0,0,0,0,0,0,0,0,0], [ 1,1,1,1,1,1,1,1,1], [2,2,2,2,2,2,2,2,2], [ 3,3,3,3,3,3,3,3,3], [ 4,4,4,4,4,4,4,4,4], [ 5,5,5,5,5,5,5,5,5], [ 6,6,6,6,6,6,6,6,6], [ 7,7,7,7,7,7,7,7,7], [ 8,8,8,8,8,8,8,8,8]]) #plt.colorbar() plt.title(r\u0026#39;Winning probabilities for black for all starting positions (Leela v0.11.0)\u0026#39;,fontsize=18) s1 = ax.scatter(x_array,y_array,c=b,s=3000,cmap=\u0026#39;rainbow\u0026#39;) for i in range(0,9): for j in range(0,9): s5 = ax.text(i, j, str(b[i,j]),horizontalalignment=\u0026#39;center\u0026#39;, fontsize=12,color=\u0026#39;white\u0026#39;,fontweight=\u0026#39;bold\u0026#39;)   ","description":"Heatmap of probability for black to win for all starting positions (Leela V0.11.0)","id":30,"section":"posts","tags":["Visualization","Data","Python","Machine Learning","Go","Baduk"],"title":"Winning probabilities in Go (leela)","uri":"https://thebooort.github.io/posts/baduk-go-winning-probabilities/"},{"content":"Image credit: La Tercera\n  Counter Strike and Data Science 1: Grenades and other utilities Counter Strike Global Offensive (CSGO) is a first person shooter game developed by Valve and Hidden Path Entertainment.\nThe game pits two teams against each other: the Terrorists and the Counter-Terrorists. Both sides are tasked with eliminating the other while also completing separate objectives. The Terrorists, depending on the game mode, must either plant the bomb or defend the hostages, while the Counter-Terrorists must either prevent the bomb from being planted, defuse the bomb, or rescue the hostages.\nIn competitive mode, 30 rounds are played, first team to get 16 victories wins.\nDuring COVID19 quarentine some friends invite me to play for the first time, and, despite I am not even close to good at it, I have a lot of fun playing it. After my first 10 wins, when I understood the game strategies a little bit more, I began to wonder if my positioning were good or if I was able to use grenades and other utilities correctly. So I decided to approach possible answers to this question with data science.\nIn this first part I\u0026rsquo;m going to analyze where and how to throw grenades, as I do not use them at all.\nUtilities (data from eSports team Dignitas )\nThe grenades that I am going to analyze are:\n The Flashbang  The flashbang, or flash, has the primary use of blinding opponents. If people look directly at it, teammate or not, the person who looked at it will be blinded for a period of time, defined by how close the flash popped and how directly the person looked at it.\n The Smoke Grenade  The smoke grenade instantly creates a thick, medium-sized (288 units diameter, or 144 units radius) smoke screen that lasts for 18 seconds after the fuse expires. The smoke screen is largely opaque except at the corners, blocking players\u0026rsquo; vision. If a player enters the smoke, the player\u0026rsquo;s weapon model will fade slightly and all vision will be obscured, even at zero range.\n The Molotov/Incendiary Grenade  This grenade explodes after a certain amount of time has passed after you have thrown it. If it explodes on, or close to, the ground (or any other flat surface) it will cover a good amount of space in flames. Everyone standing in those flames with 100 HP has roughly three and a half seconds left to get out of it before they die.\n Interaction between grenades:\nIt should be noted that you can put out any flames caused by an incendiary or a molotov with a smoke grenade, which may allow the CTs to survive longer than intended. There are no other interactions between any of them.  There more grenades, but I have been told that these are the ones that I should master first.\nGetting the data The map I am currently playing in competitive CSGO is inferno. It is not the most common map, but definetly top 5 maps. There are not specific reasons for this choice: my friends know this map relatively well and they know some strategies, so when I decide to play with them they choose this map.\n  de_inferno competitive map structure     Data from my game   I tried different ways to get my data from the game. But, at the end, most of the csgo stats webpages offer a huge amount of information with only the game_id. I realize that this was the easiest way to get my data. Meanwhile as I have access to other csgo datasets I can develop the code needed to plot and analyze all csgo data in case I am able to get my stats in the future. Sadly, for this part, I realize that I do not throw any grenade during the game (couple of smokes) so I will add them for visualization purpouses but they wont give as any information.\n  Data from professional players   With my personal data I can analyze my plays, but it is hard to judge them. I realized that comparing them with the usual plays that people in competitive mode would offer me more information. For this pourpose I find a dataset with ~ 1400 competitive matches, in kaggle obiously. That data is well formated and it is quite easy to work with.\nLocation names in Inferno In order to manage a common vocabulary and to understand my conclusions and analysis here you have the location names in the map (from (https://guides.gamepressure.com/counter_strike_global_offensive/guide.asp?ID=43283)):\n  Analyzing professional players data Heatmaps: You have to take into account that \u0026ldquo;radar\u0026rdquo; or \u0026ldquo;in-game\u0026rdquo; coordinates are not useful in order to plot your whateverstats-heatmap. For that purpose you have to scale the data into your image resolution. Beware that some points are beyond your image (for example some smokes are thrown away outside the map, and that could make some data noisy.\nFor de_inferno map, you have to make slighty deviations from a code I founded. Also you have to find the exact measures of your map, check the link in the code for that :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  # Code modified from the one used by billfreeman44 | See his kernel : https://www.kaggle.com/billfreeman44/finding-classic-smokes-by-t-side-on-mirage  # map data: https://github.com/akiver/CSGO-Demos-Manager/tree/master/Core/Models/Maps def pointx_to_resolutionx(xinput,startX=-2222,endX=3561,resX=1160): sizeX=endX-startX if startX \u0026lt; 0: xinput += startX *(-1.0) else: xinput += startX xoutput = float((xinput / abs(sizeX)) * resX); return xoutput-15 def pointy_to_resolutiony(yinput,startY=-1649,endY=4408,resY=1250): sizeY=endY-startY if startY \u0026lt; 0: yinput += startY *(-1.0) else: yinput += startY youtput = float((yinput / abs(sizeY)) * resY); return resY-youtput-120   I change a little bit the scale and exes elevation. The results were pretty good and coherent. In this situation, it is necesary to plot the map below the plots, otherwise data could not be easy to analyze:\n1 2 3 4 5 6 7 8 9 10  # Code modified from the one used by billfreeman44 | See his kernel : https://www.kaggle.com/billfreeman44/finding-classic-smokes-by-t-side-on-mirage  im = plt.imread(\u0026#39;../input/de_inferno.png\u0026#39;) plt.figure(figsize=(15,15)) t = plt.imshow(im) t = plt.scatter(csgo_sm_small[\u0026#39;grenade_xpos\u0026#39;], csgo_sm_small[\u0026#39;grenade_ypos\u0026#39;],alpha=0.05,c=\u0026#39;blue\u0026#39;) t = plt.scatter(csgo_sm_small[\u0026#39;att_xpos\u0026#39;], csgo_sm_small[\u0026#39;att_ypos\u0026#39;],alpha=0.05,c=\u0026#39;red\u0026#39;) # we can also plot trayectories for i in range(1,len(grenade_number)): t = plt.plot([a_x[i],nade_x[i]],[a_y[i],nade_y[i]],c=\u0026#39;y\u0026#39;,alpha=0.25,linestyle=\u0026#39;dashed\u0026#39;)   With this simple steps we can get a lot of information about this map and how grenades are thrown. We will analyze the data from both team, as their strategies are different.\nCounterTerrorist side In this map counterterrorist (CT) must either prevent the bomb from being planted or defuse the bomb. They can also win if they eliminate the other team and no bomb is planted.\nFor that reason, CT in most cases, want to impede Terrorist(T) to enter to A o B zone (see the previous map if you are not familiar with those names).\nCounterTerrorist flash grenades First we have flash grenades. Here you have two plots, one with thrower and grenade final location and one with the trajectories (intensity of color can be related to the commoness of the data, more intensity means more common to appear in the dataset):\n  Flash grenades locations (blue) and Thrower location (red)     Flash grenades locations (blue) and Thrower location (red)   Seeing both plots, the most used flash are thrown to Middle an Banana, in order to difficult Terrorist advance. One should take into account those which are thrown in apartments from inside LongHall and from Balcony and Pit. These last positions are the most common for me, so I take note about these flashes.\nAlso there are some interesting flashes thrown from Garden into Sanbags and Porch, mostly to blind Terrorist if they are reaching B. There can be seen two classic postitions in the map for CT, one covering Close side, and other in Patio.\nCounterTerrorist incendiary grenades Again, I am using two plots, one with thrower and grenade final location and one with the trajectories (intensity of color can be related to the commoness of the data, more intensity means more common to appear in the dataset):\n  Incendiary grenades locations (blue) and Thrower location (red)      Incendiary grenades locations (blue), tajectory (yellow) and Thrower location (red)    Most incendiary grenades are thrown in Banana to stop Terrorist to reach B. There are also some representative throws from middle, but this position is quite unsafe for CT (they are too exposed to long-range weapons in Middle ) so I am puzzle about this specific play.\nFrom my perspective, most incendiary grenades thrown into apartments came from Point A, and Balcony. I can\u0026rsquo;t move that far during the game, so I should focus myself on thrown only when I am inside Balcony.\nCounterTerrorist smoke grenades   Smokes grenades locations (blue) and Thrower location (red)     Incendiary grenades locations (blue), tajectory (yellow) and Thrower location (red)    Smokes are usually thrown into middle, I guess that if Terrorist can\u0026rsquo;t reach Point B, them they are forced to go Point A, and CT deploy a smoke grenade to being able to create some advantages in these situations.\nIn Banana, most smokes are thrown near Sandbags, which I think are mostly a late game strategy to slow down Terrorist or a deceive strategy, making Terrorist unable to see Incendiary grenade fire.\nTerrorist side In this map terrorist (T) must plant the bomb. They can also win if they eliminate the other team and no bomb is planted.\nFor that reason, T in most cases, want to access as fast as possible to either Point A or Point B (there is limited time every round and if they do not do anything in this time, they lose).\nTerrorist flash grenades   Flash grenades locations (blue) and Thrower location (red)     Incendiary grenades locations (blue), tajectory (yellow) and Thrower location (red)   Talking about Flash grenades, T usually throw a lot of these grenades on Banana. I guess that if CT do not throw any fire, T can get some advantage blinding long-range weapons that can be on porch, end of Banana, or even Logs.\nAnother hot point is end of Middle, those grenades are useful to counter the common smoke used by CT. With those grenades T can try to eliminate anyone on Close or Patio.\nFinally, on apartments, there is some throws from outside to Window (but Incendiary are more common than flashes), and other throw to Pit and Balcony, as a way to enter safely into Point A.\nTerrorist incendiary grenades   Incendiary grenades locations (blue) and Thrower location (red)     Incendiary grenades locations (blue), tajectory (yellow) and Thrower location (red)   Incendiary grenades on T side are used in different points but with similar expected outcomes.\nFrom balcony to Pit: those are used to kill enemys at Pit.\nFrom AltMiddle to Window: prevent CT to enters this room and kill any T in this street.\nFrom Banana: stop any rush from behind when T go to PointA\nFrom Sandbags to Garden: Securing PointB and stopping any player on Garden.\nTerrorist smoke grenades   Smokes grenades locations (blue) and Thrower location (red)     Incendiary grenades locations (blue), tajectory (yellow) and Thrower location (red)   Finally smokes are used by T as a way to stop or slow down CT when they are on different positions. A lot of thes smokes prevent long range weapons to attack from middle. On other side, a lot of smokes are also played creating a barrier between Well and PointB, so CT comming this way are stopped. On pointA there some relevant smokes too, securing pointA when T have reached it. Conclusions! So far so good! I definetly have to train how to throw smokes and where to throw them, but now I have a general idea of what smokes are usually thrown (despite 1vs1 situations). Can\u0026rsquo;t wait to try those in real games.\nNext steps: find hot spots in the map where I die most, and from where I am killed. See you in chapter II!\n","description":"Analyzing data from my last Counter Strike (FPS) match and using it to improve","id":31,"section":"posts","tags":["Visualization","Data","Python","Machine Learning","CSGO"],"title":"Data Science in my last CS:GO match (1)","uri":"https://thebooort.github.io/posts/analyzing-data-from-my-last-csgo-match/"},{"content":"Image credit: CDC Unsplash\nEstimando R_0 de Granada (ritmo de contagio) en ventanas semanales En lugar de hacer estimaciones o predicciones (aún creo que tengo que seguir leyendo más sobre el tema para lanzarme) he optado por analizar el ritmo reproductivo básico.\n(de Wikipedia) En epidemiología, el número básico de reproducción (a veces llamado ritmo básico de reproducción, ratio reproductiva básica y denotadas por $R0$, r sub-cero) de una infección es el número promedio de casos nuevos que genera un caso dado a lo largo de un período infeccioso.\nEste número es importante porque permite darnos una idea de cómo está evolucionando el virus y si está siendo útil las normas de confinamiento. Además nos ayuda a evaluar la evolución de la pandemia: cuando el numéro baje de 1 tendremos un virus que se contagia a menos de una persona, es decir que tenderá a desaparecer en el tiempo.\nAsí pues, vamos a analizar el $R_0$ en la provincia en la que resido durante esta epidemia: Granada.\nGetting the data A la hora de obtener los datos, he ido directamente a la página de la Junta de Andalucía. Es cierto que la página está un pelín atrasada, pero es la única fuente oficial que he encontrado que permite personalizar los datos para obtener los de Granada en particular.\nLa descargar de datos es bastante sencilla, y al poco tengo un csv con los casos nuevos, los ingresos en UCI y las muertes.\nEn nuestro caso, vamos a usar los casos nuevos. Evidentemente tenemos sesgos debido a que nuestro país no esta realizando test masivos, aún así, para evaluar la pandemia son interesantes.\nTambién tenemos que tener en cuenta que Granada es una ciudad típicamente universitaria, y que a poco de comenzar el confinamiento, muchos universitarios han regresado a su casa. Esto creo que también ayuda a la evolución de la enfermedad en esta provincia.\nAnalyzing the data Para anailzar los datos vamos a cambiar de Python ( mi lenguaje habitual) a R. ¿Por qué? Bueno esencialmente por comodidad. Realmente R es un lenguaje bastante sencillo de manejar y posee un gran cantidad de paquetes que hacen muy sencillo el análisis estadístico de datos de variados orígenes.\nEn nuestro caso vamos a usar EpiEstim, paquete que por ejemplo, también está usando ElPais (un periodico español) en sus análisis. EpiEstim se encuentra en los repositorios oficiales de R y podéis encontrar el artículo que lo presenta aqui: A New Framework and Software to Estimate Time-Varying Reproduction Numbers During Epidemics Anne Cori, Neil M. Ferguson, Christophe Fraser and Simon Cauchemez American Journal of Epidemiology 2013.\nEste paquete se centra en el estudio de $R_t$, esat constante es el número reproductivo instantaneo, que se puede estimar mediante el número de nuevas infecciones generadas en tiempo $t$, frente al total de infecciosidad de los individuos contagiados en tiempo $t$. Este total se calcula siguiendo un sumatorio que usa la distribución de probabilidad que sigue la enfermedad ( que suponemos normal) y la incidencia en $t-1$. En resumen, $R_t$ es la media de casos secundarios que cada individuo contagiado puede provocar si las condiciones se mantienen en tiempo $t$.\nHacer inferencia sobre esta variable puede ser complicado por su alta variabilidad. Por eso, el paquete plantea hacer inferencia de la misma en ventanas temporales que se solapan, para aportar mayor significación a las estimaciones.\nLo último que necesitamos para hacer nuestra predicción es el intervalo serial (serial interval en ingles). El intervalo serial es el nombre que recibe el tiempo que transcurre entre los casos sucesivos en una cadena de transmisión.\nHabitualmente se estima generalmente a partir del intervalo entre los inicios clínicos, en cuyo caso es el \u0026ldquo;intervalo de serie de inicio clínico\u0026rdquo; cuando estas cantidades son observables. En principio, podría estimarse por el intervalo de tiempo entre la infección y la transmisión posterior. En el caso particular del covid he usado esta referencia:\n Serial interval of novel coronavirus (COVID-19) infections. Hiroshi Nishiura, Natalie M. Linton,Andrei R. Akhmetzhanov  Con lo que el intervalo será de 4.6 días con media de 4.8 días y desviación típica de 2.3 días.\nPlot Obtenemos finalmente el gráfico. Como véis he representado 3 datos esenciales para nuestro análisis. Por una parte un histograma básico con el número de casos que han acontecido estos días (desde que fueron representativos, recordemos que Granada tuvo 0 casos hasta muy entrada la pandemia en muchas provincias).\nEn segundo lugar aparece la estimación del ritmo reproductivo básico a lo largo del tiempo. Aunque no se aprecie posee los rangos superior e inferior del mismo. Y finalmente se muestra cual es el intervalo serial escogido como hemos comentado en la sección anterior.\nConclusions! Parecen buenas noticias! Hemos pasado ya a un estado con $R_0\u0026lt;1$, aunque son buenas noticias (implicarían que a la larga la enfermedad desaparecería) hay que tener en cuenta que esto se debe a las medidas de confinamiento y que debemos ser muy cautos aún.\nDeberíamos bajar este término todo lo posible antes de acabar con el confinamiento, para reducir lo máximo posible los rebrotes.\nA\n","description":"Estimando R_0 de Granada (ritmo de contagio) en ventanas semanales","id":32,"section":"posts","tags":["Visualization","Data","COVID19","R","Statistics","Mathematics"],"title":"¿Cómo vamos? Estimando el ritmo de contagio ","uri":"https://thebooort.github.io/posts/r_0granada/"},{"content":"Image credit: Ivan Aleksic @ivalex Unsplash\n¿Cómo afecta el confinamiento a los niveles de polución? Durante la cuarentena que estamos viviendo la reducción de la movilidad de las personas es evidente.\nLigada a esta disminución del tránsito de vehiculos particulares, vehiculos de transporte público y algunas maquinarias pertenecientes a servicios que se encuentran inactivos, muchos medios se han hecho eco de la disminución en los niveles de polución ambiental. Es más, al ser COVID una enfermedad que afecta a las vías respiratorias, ya hay algunos analisis que relacionan la polución del aire con tasas de moratilidad mas altas por COVID.\nPor este motivo me pareció interesante estudiar el caso de la ciudad en la que resido: Granada(España).\nLa evidencia muestra que los niveles de polución han bajado, pero ¿cómo sabemos esto?.\nPara contestar a esto vamos a tomar dos enfoques: primero, conocer qué buscamos en el aire para mediar su calidad, y, segundo, conocer alguna técnica que nos ofrezca los datos necesarios para nuestro análisis.\n¿Qué buscamos? La calidad del aire se mide en relación a la presencia y cantidad de determinados contaminantes en el aire. Los principales contaminantes monitorizados suelen estar determinados por la legislación vigente, que también determina qué baremos se utilizan para evaluar cada uno.\nAun así, hay una gran variedad de contaminantes sobre los cuales existe un conseso (en cuanto a su peligrosidad) y que son monitorizados habitualmente en estaciones medidoras de calidad del aire (habitualmente en ciudades). Algunos ejemplos son:\n $NO_2$ y en general los óxidos de nitrógeno. Estos se producen como consecuencia del uso de combustibles fósiles como petróleo, carbón o gas natural. Es un contaminante muy perjudicial. $CO_2$ y $CO$ su origen antropogénico es debido a la combustión incompleta de materias orgánicas (gas, carbón, madera, etc.), en especial los carburantes de los automóviles.  A estos habría que sumarles otros como las particulas en suspensión ($PM_5$ o $PM_10$ según su tamaño), el Ozono, el Dióxido de azufre, etc.\nDespúes de consultar diversas páginas y leer las noticias sobre polución y COVID, me quedaba claro que sería muy interesante comprobar los niveles de NO_2, pues tienen una clara relación con las actividades de transporte y su uso está muy extendido como parámetro que mide la calidad del aire.\nGetting the data ¿Cómo obtenemos los datos? Esencialmente los datos sobre los contaminantes se pueden encontrar en internet vía dos posibles orígenes.\nPor una parte muchos de estos datos se originan en estaciones de medición locales. Estas permiten una obtención de datos directamente de nucleos urbanos particulares, con una cantidad de datos relativamente pequeña. La parte negativa es que dependes de los sensores de la estación. En mi caso, Granada tiene varias estaciones, pero por ejemplo no pude encontrar datos de Ozono o de las partículas en suspensión.\nLa otra opción es usar datos de satélite. Aunque hay datos bastante actualizados sobre estas mediciones, habitualmente pesan bastante (puesto que se parten solo regiones grandes como Europa o America del Norte), y ahora mismo mi conexión de internet no da para tanto.\nParticularizando esta última opción, si os véis valientes en Europa tenempos el TROPOMI, un instrumento de monitorización de la trposfera que tiene datos de NO2 que forma parte del Sentinel-5 de la Agencia Espacial Europea.\nFinalmente, para mi análisis me dicidí por usar datos de las estaciones locales, aunque solo encontré de NO2, al menos son datos oficiales y de un volumen manejable para tratarlos. Como extra también tuve acceso al histórico de otros años, para así compararlo. Toda la información la saqué de la Agencia Medioambiental Europea.\nAnalyzing the data La verdad es que los datos no estaban en la mejor versión posible. Además fue un poco lioso conseguirlos. Aún así, con un poco de preprocesado obtuve lo que buscaba.\nLo único a destacar durante esta fase es la conversion a dato tipo fecha que necesitas para plotear datos pertenecientes a series temporales:\n1 2  no2_df_2020[\u0026#39;date\u0026#39;] = pd.to_datetime(no2_df_2020[\u0026#39;date_time\u0026#39;]) no2_df_2019[\u0026#39;date\u0026#39;] = pd.to_datetime(no2_df_2019[\u0026#39;date_time\u0026#39;])   En este aspecto ospuede ayudar strftime, pues que en muchas ocasiones os vais a encontrar fechas en formatos muy diferentes. Es habitual tambien que algunas librerías requieran el formato de año-mes-dia , o mes-dia-año, para todas estas ocasiones con strftime podéis salir del paso.\n(El ejemplo es de otro código pero así os haceís una idea de como usarlo)\n1  df[\u0026#39;date\u0026#39;] = pd.to_datetime(df[\u0026#34;date\u0026#34;].dt.strftime(\u0026#39;%d/%m/%Y\u0026#39;))   Y bueno, como ya sabéis empecé a usar bokeh hace poco, asi que esto era una situación perfecta para poner a prueba mis conocimientos.\nPlot  He marcado con verde ( no lo había usado antes) el nivel de contaminante que la OMS considera seguro (en media al año).\nPara ello he usado la funcionalidad BoxAnnotation, que además puede personalizarse un monton y se puede usar con fechas de inicio y fin:\n1 2 3 4  low_box = BoxAnnotation(top=40, fill_alpha=0.1, fill_color=\u0026#39;green\u0026#39;) top_box = BoxAnnotation(bottom=40, top=80, fill_alpha=0.1, fill_color=\u0026#39;red\u0026#39;) p1.add_layout(low_box) p1.add_layout(top_box)   Añadí el marcaje del día en el que se inicia la cuarentena. Además saqué la leyenda fuera, porq dentro del gráfico molestaba bastante:\n1 2 3 4 5 6 7 8  from bokeh.models import Legend legend = Legend(items=[ (\u0026#34;2020\u0026#34; , [r1,r2]), (\u0026#34;2019\u0026#34; , [r3, r4]), ], location=\u0026#34;center\u0026#34;) p1.add_layout(legend, \u0026#39;right\u0026#39;)   Conclusions! Si bien es cierto que en abril hay un descenso en los dos años (puede que la lluvia ayude), veníamos de un febrero muy caluroso y sin apenas lluvia, por lo tanto veníamos de una situación negativa. También se puede observar que ya de por sí nos encontrabamos en un punto muy bueno, menor al del año pasado. Me llama la atención un pequeño repunte (quizas por el mayor uso de coches unipersonales para ir al trabajo en lugar de transporte público?), con todo, estamos lejos de los niveles del año pasado. No olvidemos tampoco que estamos representando un promedio semanal, con lo que captamos eventos que han podido pasar durante la semana completa.\nA mitad del encierro hay una gran bajada que nos deja en un nivel muy bajo y posiblemente relacionada con la mayor restricción de circulación. Llegamos a un punto muy inferior a otros años por la misma fecha, lo que parece indicar que sí que se debe a causas antropológicas. Además es un buen indice de que la gente se está tomando en serio la cuarentena :) y que aunque sea por unos días, estamos respirando un aire más limpio.\n","description":"Evolución de los niveles de polución en Granada durante la cuarentena","id":33,"section":"posts","tags":["Visualization","Data","COVID19","Python","Pollution","Ecology","Air Quality"],"title":"El aire que respiramos durante el confinamiento","uri":"https://thebooort.github.io/posts/pollution_levels/"},{"content":"Image credit: Javier Ezpeleta @javierezpeleta Unsplash\nDid confinement affects my sleep routine? Llevamos ya varias semanas de cuarentena y los ánimos varían mucho de un día a otro. Ultimamente han aparecido muchos artículos y estudios sobre el posible impacto del confinamiento en nuestros ritmos de vida. El tema me despertaba bastante curiosidad desde el principio, y da la casualidad de que por mis otros hobbies (trail running) tengo un reloj capaz de captar algunas de mis rutinas. En particular el reloj puede realizar mediciones de sueño, actividad física, pasos, etc.\nEvidentemente la actividad física se vió afectada por el confinamiento, reduciendo mis ejercicios y mis pasos diarios. Para que os hagáis una idea mis pasos han pasado de 14.149,2 de media diaria a sólo 1869,1.\nSi bien es cierto que cuando más ando, al salir a comprar para varios días, no me llevo el reloj para evitar tocarlo con las manos sucias, actualmente salgo 1 vez cada semana y media, con lo cual creo que no ayudan a la media en absoluto.\nTambién hay que tener en cuenta que al hacer ejercicio en casa (algo que hago entre 1 hora / 1hora y media al día), con los movimientos que practico, el reloj también cuenta pasos. Aún así, creo que gracias a eso solo he llegado un día a la cifra de 4000 pasos.\nEn resumen, creo que el parámetro del que tengo menos idea es el del sueño, y es uno de los que destacan para evaluar mi posible productividad, descanso, concentración, o incluso estado anímico.\nGetting the data Para obtener los datos tiré de GPDR y solicité mis datos a la empresa de la cual viene mi reloj deportivo. Tardarón un par de días en darme los datos, pero a su favor, gestionaron bastante bien mi solicitud.\nLos datos se distribuyen de la siguiente manera (de su página web):\n  Sueño profundo\nCuando pasas a la fase de sueño profundo, los movimientos oculares y musculares se detienen por completo. La frecuencia cardiaca y el ritmo de tu respiración disminuyen.\n  Sueño ligero\nEl sueño ligero es la primera fase del sueño. Durante la fase de sueño ligero, los movimientos oculares y la actividad muscular se ralentizan.\n  Despierto\n  Para detectar estas fases el reloj hace uso del acelerómetro y el pulsómetro. En aquellos dispositivos con oxímetro también es tenido en cuenta, pero no es mi caso.\nAnalyzing the data Finalmente con los datos obtenidos, procedo a usar bokeh para un EDA. Dentro del mismo gráfico incluyo los datos de sueño total, de sueño profundo y sueño ligero.\nAdemás de los datos en crudo, he marcado la franja de sueño saludable recomendada (7-8h). Como parte del análisis, he realizadao una pequeña regresión para intentar ver qué tendencia seguían los datos.\nSin más información lo cierto es que son pocos datos y no es del todo representativa, pero creo que se puede intuir alguna conclusión de ella.\nConclusions! Bueno viendo los datos soy optimista. He seguido de forma adecuada un rutina de sueño para evitar alterar mis ciclos y ritmos diarios.\nEn parte estoy acostumbrado a realizar mis principales entrenamientos por la tarde-noche durante la semana. Estos horarios, al mantenerlos creo que me han ayudado a llegar cansado a la hora de dormir.\nEs cierto que se intuye un leve subida de las horas de sueño, pero con menor cantidad de sueño profundo. Esto puede afectar a la calidad del sueño, pero las variaciones son tan sumamente mínimas, que no creo que tenga que preocuparme.\n","description":"Analyzing sleep patterns before and during COVID19 quarentine","id":34,"section":"posts","tags":["Visualization","Data","COVID19","Python","Machine Learning"],"title":"Did confinement affect my sleep/exercise routine?","uri":"https://thebooort.github.io/posts/analyzing-sleep-patterns-before-and-during-covid19-quarentine/"},{"content":"Bar chart race Siguiendo un poco el interés que me suscita la representación visual de datos, aprovechando la cuarentena me puse a descubrir la mejor forma de hacer una \u0026ldquo;carrera de gráficos de barras\u0026rdquo;.\nEste tipo de visualización ha cogido fama últimamente. Más que por su su utilidad a la hora de analizar los datos, por su forma atractiva y visual de presentar la información, haciéndola muy apetecible al público en general y prestándose a ofrecer videos interesantes fáciles de consumir.\nPor esta razón, y aprovechando para adquirir nuevos conocimientos que me pueden ser útiles en el futuro, me puse a replicar este tipo de visualización gráfica. El objetivo de estos posts no es solo aprender herramientas nuevas o rebuscar códigos interesantes, también ser capaz de ofrecer un gráfico final vistoso que sea consumible.\nOpciones Buscando por internet encontré varias opciones para relizar este tipo de gráficos.\n Python: Python siempre sale a relucir sea cual sea tu requerimiento. En esta ocasión encontré posts en TowardsDataScience donde te explicaban paso a paso como conseguir el efecto. Aún así, los resultados que veía, si bien per se eran interesantes, no llegaban al resultado final que estaba buscando. Tableau: Tableau es otro clásico de la visualización utilizado por muchas empresas con una opción gratuita en Tableau Public con un gran potencial para realizar gráficos interactivos de todo tipo y con buen acabado. Algunos de los ejemplos que ví con esta herramienta no me llegaron a captar la atención, por lo que decidí dejar su aprendizaje para más tarde (tengo pendiente ponerme con él). Flourish: Llegamos finalmente a Flourish. Flourish es otra herramienta que se puede gestionar online que ofrece muchas y variadas visualizaciones. Encontré esta herramienta debido a una de las gráficas que se publicaron en un periódico online. Ojeándola me gustó mucho su acabado y decidí darle una oportunidad para el objetivo que tenía.  Datos y preprocesado Una vez que escogí Flourish, quedaba elegir qué datos quería representar. Como llevaba tiempo trabajando con casos de COVID-19 decidí alejarme un poco de esta dinámica y buscar alguna fuente de datos interesante.\nCasi por casualidad ese día estuve comentando los datos sobre las disminuciones de contaminación atmosférica de las grandes ciudades debido al confinamiento. Asi que las emisiones de $CO_2$ rondaban mi mente, y aunque había relación con el COVID19, el aspecto ecológico me atraía bastante.\nEncontrar dataset buenos sobre cualquier tema es complicado, más aún temás ecológicos/climáticos. Por suerte en ensta ocasion cuento con OurWorldInData fuente de datasets de todo tipo con muchisima información y bien formateados. Decidí elegir las emisiones de $CO_2$ por persona. Esencialmente, para calcular la contribución del ciudadano promedio de cada país a las emisiones totales de $CO_2$ se dividen sus emisiones totales por su población, obteniendo las medidas en tonelaadas por usuario por año.\nEstas emisiones juegan un papel fundamental dentro de los objetivos climáticos de los años venideros y, como dicen en la web: gracias a las reconstrucciones históricas, están disponibles para todo el mundo desde mediados del siglo XVIII.\nEsto nos permite captar tendencias globales, casos concretos con cifras muy altas y analizar quienes son los principales emisores de $CO_2$\nUna vez descargamos los datos y los exportamos a Flourish, tenemos un par de problemas:\n  Formato de los datos: Los datos se encuentran por fecha y pais, pero, para repesentarlos como queremos necesitamos que se muestren como una serie temporal. Con ese fin, simplemente mapeamos los distintos años y creamos columnas con cada país.\n  Datos perdidos y nuevos paises: Algunos países no poseen todos los datos, en estos casos hemos optado por hacer una media entre el año posterior del que tenemos datos y el año anterior del cual tenemos datos. No es una solución ideal, pero como este tipo de gráfico no acaba representando toda la información, mas tarde nos cercioramos de que ninguno de los países en los cuales hicimos este procesado sale en el gráfico final. Para aquellos países que no tienen datos hasta un determinado año, la información se completa con ceros.\n  Una vez solventados estos dos problemas, toqueteamos las opciones de Flourish hasta que nos convenza el diseño.\nResultado final Grafico interactivo Este es el resultado final. Un gran acabado, muy contento con el resultado ¡y es interactivo!\n Video Este tipode gráficos también quedan vistosos en video, aquí os dejo el resultado con musiquita:\n  Conclusiones curiosas Por un lado destacan aquellos paises que son grandes productores de crudo, sobre todo aquellos con una población pequeña (Qatar , Kuwait\u0026hellip;). Por otra parte destaca un pico en Brunei a mediados del siglo XX, este pico da para mucho y debería escribir un post sobre él.\nDicho esto, creo que los resultados son mas que satisfactorios, una herramienta interesante y habilidades de manejo que pueden serme útiles si necesito tirar de graficos animados e interactivos con formatos que otras librerías no tienen (o cuesta mucho implementar).\n","description":"Probando Flourish para hacer graficos dinámicos","id":35,"section":"posts","tags":["Flourish","Visualization","Data","Bar Chart Race"],"title":"CO₂ emissions per capita in history: a bar chart race","uri":"https://thebooort.github.io/posts/bar_chart/"},{"content":"COVID19 Kaggle Challenge Dentro de los challenges lanzados por Kaggle para usar los datos publicados del COVID19, los doctorandos que compartimos directora de tesis nos juntamos para probar ideas y aprender lo que pudiésemos en el camino.\nHabía varios retos o tareas en las que podíamos embarcarnos, pero como nuestro interés era más bien académico, decidimos empezar haciendo un análisis exploratorio de datos y clusterización. Y desde ahí, lo que quisiésemos o nos diese tiempo.\nLos resultados que véis aquí corresponden a un enfoque muy sencillo y básico de aplicación de técnicas de machine learning para crear un recomendador de artículos científicos relacionados con uno seleccionado por el usuario.\nEl objetivo es usar los titulos y abstracts de los artículos para extraer \u0026ldquo;de qué van\u0026rdquo;. Una vez obtenido esto, se buscan artículos que traten temas similares.\nPara ello seguimos estos pasos:\n EDA Text Preprocessing Tokenization vectorization LDA and Topic Extraction Recommendation system  Si quereis echarle un vistazo a el notebook (aunque esta desordenado): https://www.kaggle.com/thebooort/covid19-exploration-and-paper-recommendation\nEste post pretende presentar los principales resultados del notebook (hasta que encuentre una forma de subir los notebooks enteros), omitiré trozos de código que considere poco importantes.\nEDA En este aspecto, trabajando con abstract y texto, tendremos que saber cuantas palabras realmente van a entrar a nuestro sistema. Para ello una diagramas de cajas puede sernos útil:\n1 2 3 4 5  df[[\u0026#39;abstract_word_count\u0026#39;]].plot(kind=\u0026#39;box\u0026#39;, title=\u0026#39;Boxplot of Word Count\u0026#39;, figsize=(10,6)) plt.show() df[[\u0026#39;body_word_count\u0026#39;]].plot(kind=\u0026#39;box\u0026#39;, title=\u0026#39;Boxplot of Word Count\u0026#39;, figsize=(10,6)) plt.show()   Además, una nube de palabras tambien puede darnos una idea aproximada de qué palabras tienen importancia (tanto aquellas que son importante como las que no).\n1 2 3 4 5 6 7 8  for key in [\u0026#39;abstract\u0026#39;,\u0026#39;title\u0026#39;,\u0026#39;full_text\u0026#39;]: total_words = df[key].values wordcloud = WordCloud(width=1800, height=1200).generate(str(total_words)) plt.figure( figsize=(30,10) ) plt.title (\u0026#39;Wordcloud\u0026#39; + key) plt.imshow(wordcloud) plt.axis(\u0026#34;off\u0026#34;) plt.show()   Tokenización Para esta seccion siguiendo un código de ajrwhite, usamos scapy. La idea es tokenizar el texto, no solo para esta parte, si no para cualquier analisis o algoritmo posterior, quedandonos con un texto depurado y palabras que realmente sean importante para etiquetar, clasificar, o mdelizar estos textos.\n1 2 3 4 5  # importamos scispacy un paquete para trabajar spaCy con textos científicos y usamos el modelo # en_core_sci_md que contine pipeline de spacy especifica para textos biomédicos (50k palabras) import en_core_sci_md nlp = en_core_sci_md.load(disable=[\u0026#34;tagger\u0026#34;, \u0026#34;parser\u0026#34;, \u0026#34;ner\u0026#34;]) nlp.max_length = 2000000   Del word cloud tenemos algunas sugerencias de stop words que no son relevantes en nuestro análisis.\nAñadiéndolas lanzamos finalmente la tokenizacion y la aplicamos sobre nuestras columnas de texto.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  customize_stop_words = [ \u0026#39;doi\u0026#39;, \u0026#39;preprint\u0026#39;, \u0026#39;copyright\u0026#39;, \u0026#39;peer\u0026#39;, \u0026#39;reviewed\u0026#39;, \u0026#39;org\u0026#39;, \u0026#39;https\u0026#39;, \u0026#39;et\u0026#39;, \u0026#39;al\u0026#39;, \u0026#39;author\u0026#39;, \u0026#39;figure\u0026#39;, \u0026#39;rights\u0026#39;, \u0026#39;reserved\u0026#39;, \u0026#39;permission\u0026#39;, \u0026#39;used\u0026#39;, \u0026#39;using\u0026#39;, \u0026#39;biorxiv\u0026#39;, \u0026#39;fig\u0026#39;, \u0026#39;fig.\u0026#39;, \u0026#39;al.\u0026#39;, \u0026#39;di\u0026#39;, \u0026#39;la\u0026#39;, \u0026#39;il\u0026#39;, \u0026#39;del\u0026#39;, \u0026#39;le\u0026#39;, \u0026#39;della\u0026#39;, \u0026#39;dei\u0026#39;, \u0026#39;delle\u0026#39;, \u0026#39;una\u0026#39;, \u0026#39;da\u0026#39;, \u0026#39;dell\u0026#39;, \u0026#39;non\u0026#39;, \u0026#39;si\u0026#39;,\u0026#39;elsevier\u0026#39;, \u0026#39;unrestricted\u0026#39;, \u0026#39;grant\u0026#39;, \u0026#39;repository\u0026#39;, \u0026#39;original\u0026#39;, \u0026#39;acknowledgement\u0026#39;, \u0026#39;permission\u0026#39;, \u0026#39;centre\u0026#39;] for w in customize_stop_words: nlp.vocab[w].is_stop = True stop_words = nlp.Defaults.stop_words # definimos el tokenizador con las stopwords  def spacy_tokenizer(sentence): return [word.lemma_ for word in nlp(sentence) if not (word.like_num or word.is_stop or word.is_punct or word.is_space or len(word)==1)] # aplicar tokenizacion df[\u0026#39;full_text\u0026#39;] = df[\u0026#39;full_text\u0026#39;].apply(spacy_tokenizer) df[\u0026#39;abstract\u0026#39;] = df[\u0026#39;abstract\u0026#39;].apply(spacy_tokenizer) df[\u0026#39;title\u0026#39;] = df[\u0026#39;title\u0026#39;].apply(spacy_tokenizer) # eliminar espacios df[\u0026#39;full_text\u0026#39;] = df[\u0026#39;full_text\u0026#39;].apply(lambda x: \u0026#39; \u0026#39;.join(x)) df[\u0026#39;abstract\u0026#39;] = df[\u0026#39;abstract\u0026#39;].apply(lambda x: \u0026#39; \u0026#39;.join(x)) df[\u0026#39;title\u0026#39;] = df[\u0026#39;title\u0026#39;].apply(lambda x: \u0026#39; \u0026#39;.join(x))   Vectorización Vamos a vectorizar nuestra información textual ( esto es, transformar las palabras en vectores numéricos). Para ello podemos usar CountVectorizer o TF-IDF entre otros. Usé estos dos por simpleza, pero CountVectorizer me dió mejores resultados (CountVectorizer esencialmente vectoriza la palabra contando la frecuencia de repetición de esa palabra en los títulos o en los abstracts).\nImportando sklearn usar cualquiera de estos dos algoritmos es muy sencillo. Además, podemos añadir el tokenizador que queramos, en este caso el que hemos construido con spacy.\n1 2 3 4  features = 10000 tf_vectorizer = CountVectorizer(max_features=features,tokenizer = spacy_tokenizer, min_df=10) X_tf = tf_vectorizer.fit_transform(df[\u0026#39;abstract\u0026#39;]) tf_feat_name = tf_vectorizer.get_feature_names()   LDA LDA es basciamente un método estadístico que busca encontrar una combinación lineal de rasgos que caracterizan o separan dos o más clases de objetos o eventos. La combinación resultante la podemos utilizar para separar nuestros textos en grupos y obtener qué palabras aparecen con más frecuencia dentro de esos grupos, a fin de caracterizarlos.\n1 2 3  topics = 7 lda_model = LatentDirichletAllocation(learning_method=\u0026#39;online\u0026#39;,random_state=20,n_components=topics) lda_output =lda_model.fit_transform(X_tf)   Resultado final del LDA Para visualizar LDA a parte de obtener los topics, usé pyLDAvis. pyLDAvis es una biblioteca de Python para visualización interactiva de topic models (basada en el paquete R de Carson Sievert y Kenny Shirley).\nComo dicen en su web pyLDAvis está diseñado para ayudar a los usuarios a interpretar los topics en un modelo de topics que se ha ajustado a un corpus de datos textuales. El paquete extrae la información de un modelo de topics LDA ajustado, para ofrecer una visualización interactiva.\nOs lo recomiendo.\n1 2 3  # preparing for plotting pyLDAvis %matplotlib inline pyLDAvis.enable_notebook()   La visualización está diseñada para ser utilizada en cualquier notebook, pero también se puede guardar en un archivo HTML independiente para compartir fácilmente:\n1 2 3 4  pyLDAvis.sklearn.prepare(lda_model, X_tf, tf_vectorizer) # if you want to save it  # P=pyLDAvis.sklearn.prepare(lda_model, X_tf, tf_vectorizer) # pyLDAvis.save_html(p, \u0026#39;lda.html\u0026#39;)    Podéis consultar el resultado como una página aparte aqui: https://thebooort.github.io/covizzz19/ , así os quitáis de problemas si estáis viendo este blog como theme oscuro.\nVisualizando las temáticas Las temáticas finales obtenidas fueron:\nTopic 0 ['protein', 'rna', 'proteins', 'activity', 'cell', 'replication', 'antiviral'] Topic 1 ['infection', 'cell', 'immune', 'infected', 'mice', 'response', 'induced', 'expression'] Topic 2 ['respiratory', 'pcr', 'samples', 'detection', 'viruses', 'assay', 'using', 'positive'] Topic 3 ['diseases', 'disease', 'review', 'development', 'health', 'research', 'based', 'new', 'use', 'infectious'] Topic 4 ['patients', 'calves', 'il', 'group', 'associated', 'days', 'cats', 'clinical', 'age'] Topic 5 ['cov', 'sars', 'protein', 'coronavirus', 'sequence', 'mers', 'human', 'genome', 'viruses'] Topic 6 ['health', 'influenza', 'risk', 'data', 'cases', 'disease', 'outbreak'] Podemos estudiar las correlaciones entre las puntuaciones de los topics de cada paper:\n1 2 3 4  # plotting results import seaborn as sns sns.heatmap(abs(ldadf[columns].corr()),annot=True) plt.title(\u0026#34; Correlation Plot\u0026#34;)   Obtenemos una correlación baja entre topics, lo que implica que nuestra clusterizacion (si bien mejorable) no es mala.\nRecomendaciones Para obtener la recomendacion final, bastará con comparar las componentes que identifican a cada paper. Seleccionado uno cualquiera, calcularemos la distancia con el resto de papers y devolveremos los mas cercanos. Esta distancia la podemos definir como queramos: RMSE, Cosine similarity, etc.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  from sklearn.metrics.pairwise import cosine_distances from math import sqrt def paper_recommendation_tool(selected_paper_id): selected_paper = df_total[(df_total[\u0026#39;paper_id\u0026#39;]==selected_paper_id)] dominant_topic = int(selected_paper[\u0026#39;Major_topic\u0026#39;]) # we create another df with papers evaluation recommended_papers = df_total recommended_papers = recommended_papers.drop(selected_paper.index) x= selected_paper[columns].values.tolist() y= recommended_papers[columns].values.tolist() for indice_fila, fila in recommended_papers.iterrows(): distance = cosine_distances(x[0],y[indice_fila-1]) recommended_papers[\u0026#39;distance\u0026#39;] = distance recommended_papers = recommended_papers[[\u0026#39;paper_id\u0026#39;,\u0026#39;title\u0026#39;,\u0026#39;distance\u0026#39;,\u0026#39;Major_topic\u0026#39;]].sort_values(\u0026#39;distance\u0026#39;).head(10) return dominant_topic, recommended_papers   Probamos un caso concreto\n1 2 3 4  # Example: we are interested in 4602afcb8d95ebd9da583124384fd74299d20f5b, because his use of SPINT2 dominant_topic, recommended_papers = paper_recommendation_tool(\u0026#39;4602afcb8d95ebd9da583124384fd74299d20f5b\u0026#39;) print(dominant_topic) print(recommended_papers)   ","description":"COVID19 Kaggle Challenge: Construyendo un recomendador de artículos sobre COVID19","id":36,"section":"posts","tags":["Kaggle","Coding","Data Science","COVID19","Python","Machine Learning","Recommedation Systems"],"title":"COVID19 research paper recommendation engine","uri":"https://thebooort.github.io/posts/covid_kaggle/"},{"content":"¡Ojo! Perl6 ya \u0026ldquo;no existe\u0026rdquo;, oficalmente el lenguaje ahora se llama ¡Raku! :)\nPerl6 Documentation Durante un breve periodo de tiempo estuve estudiando la evolución en las contribuciones a los repositorios de código. En muchas ocasiones se dice que este tipo de contribuciones siguen leyes de potencias, pero habitualmente esta afirmación va apoyada únicamente por un par de fits de distribuciones a los datos, sin contraste de hipótesis o comparación con otras posibles distribuciones. Aunque ya me meteré en este costal más adelante, lo cierto es que analizar las contribuciones a cualquier repositorio es siempre interesante.\nY como ya sabéis, el tema de la visualización me gusta bastante, por lo que, documentándome para el trabajo que antes os mencionaba, encontré Gource.\nY como por aquel entonces también estaba escribiendo cosillas para la documentación de Perl6 (en particular su sección sobre simulación de Ecuaciones diferenciales) decidí probar a ver que pasaba al usar esta herramienta con ese repo.\nLos resultados gustaron mucho, y subieron el video a los updates semanales de Perl6 :).\nOs animo a usar Gource en vuestros proyectos de software libre, es un curioso gráfico que puede llamar la atención de los usuarios y ser útil a la hora de mostrar vuestro repo (o la actividad en el mismo).\nVideo   Data Source Repositorio de la documentacion de Perl6 a fecha de creación del video.\nSoftware usado  Gource  ","description":"Video sobre la evolución del repo de documentación de Perl6","id":37,"section":"posts","tags":["Github","Repository","Visualization","Perl6","Raku"],"title":"How to visualize a github repository evolution in time","uri":"https://thebooort.github.io/posts/perl6-viz/"},{"content":"Este post continúa la serie sobre visualización del COVID-19. La motivación para hacerla no es otra que ocupar los ratos muertos de la cuarentena para aprender a crear gráficos interactivos con Bokeh.\nY de paso ofrecer un vistazo objetivo a los datos que nos llegan sobre la enfermedad. Sé que suena un poco raro pero ya que voy a estar dándole vueltas al tema, tener este enfoque objetivo me ayuda a sobrellevarlo.\nEn cuanto depure los códigos los subo a github y tendréis el enlace aquí:\nCOVID19 - Gráficos de Europa Para cerrar (por ahora) esta introducción que me estoy haciendo a la biblioteca Bokeh, quería aportar luz sobre la evolución de la enfermedad por el mundo.\nNi por estudios ni por trabajo, nunca me ha tocado trabajar con datos geográficos. Es por eso que quería llenar un poco la laguna que tengo a la hora de representarlos gráficamente. He visto que bokeh es un excelente recurso para eso, asi que me he lanzado a sacarle partido.\nEn esta ocasión estoy siguiendo un tutorial de Shivangi Patel.\nEl tutorial está estupendo y sirve no solo como introducción para plotear mapas si no también para cómo hacerlo en Bokeh y como usar Bokeh para generar un minidashboard en el que los datos vayan actualizandose.\nEsto último, requiere de un server que lamentable mente no tengo, asi que os dejo un su lugar un video del resultado.\nPor otra parte, he aprovechado para trabajar con geopandas y empaparme un poco del tema de datos geográficos. Lamentablemente el dataset consultado usaba un GEOID un tanto raro, que no he conseguido acoplar correactamente al archivo shapes (no usa el estandar de codificacion de países de 3 letras, y no coincide con los que he visto de 2 letras), con lo que la info de algunos países a pesar de tenerla, no está visible.\nEn cualquier caso, como introducción, contento con los resultados.\nGraficos Gráfico interactivo de casos y muertes Aquí aproveche para manejar pestañas y ofrecer en el mismo grafico la posibilidad de consultar ambas variables en el mundo. Probé con los hover, pero no me encajaron mucho y decidí que esta era la mejor opción.\n Gráfico interactivo de casos y muertes Finalmente añadí la interacción al gráfico de manera que se acualice a cada paso. La verdad que el data set ha mostrado ser insuficiente, con muchos valores perdidos. No obstante, creo que el caso de Italia (podéis hacer zoom :) ) se ve bastante bien como empeora el color.\nOs dejo por aquí el video del resultado\n  Extra: El dashboard de la Johns Hopkins University Por último aprovecho para dejaros el dashboard de la Johns Hopkins University, que está muy currado y con la info al día.\n Data Source Todos los datos están obtenidos de ECDC COVID-19\nLibrerías usadas  Bokeh Pandas GeoPandas  Con esta parte creo que aparco de momento esta serie de COVID+Bokeh en pos de hacer algún post sobre otras herramientas. No obstante, no descarto volver en breve, el tema de la visualización me gusta mucho. ","description":"Graficos sobre COVID19 usando bokeh","id":38,"section":"posts","tags":["COVID19","Python","Visualization"],"title":"COVID19 + Bokeh - Mapas!","uri":"https://thebooort.github.io/posts/covid_bokeh_mapa/"},{"content":"Este post continúa la serie sobre visualización del COVID-19. La motivación para hacerla no es otra que ocupar los ratos muertos de la cuarentena para aprender a crear gráficos interactivos con Bokeh.\nY de paso ofrecer un vistazo objetivo a los datos que nos llegan sobre la enfermedad. Sé que suena un poco raro pero ya que voy a estar dándole vueltas al tema, tener este enfoque objetivo me ayuda a sobrellevarlo.\nEn cuanto depure los códigos los subo a github y tendréis el enlace aquí:\nCOVID19- Madrid vs. Andalucía Para este segundo post quise usar los datos del crecimiento en Madrid, para compararlos con Andalucía.\nLa idea aquí era ver cómo de diferentes habían sido las escaladas de casos y ver cómo quedaban al representarlas la una frente a la otra.\nDe paso, hacer hincapié en opciones para personalizar. En particular me gusta mucho la opción \u0026lsquo;mute\u0026rsquo; de la lenyenda, que te permite cambiar la opacidad de una de las líneas para visualizar mejor el resto. Os animo a probarla.\n1 2 3 4 5 6  from bokeh.plotting import figure p = figure() p.legend.click_policy=\u0026#39;mute\u0026#39; # recordad que podéis personalizar cómo actúa en vuestra gráfica p.circle(x=\u0026#39;date\u0026#39;, y=\u0026#39;cases\u0026#39;, muted_alpha = 0.2)   Aún no manejo los hovers bien (¿quizás es problema de cómo está estructurado el dataset?), asi que estoy optando por poner toda la información. Aunque repito que no me gusta el resultado.\nPor otra parte también quise ensayar la unión de dos gráficas, que quizás se veía mas explicativa. Y ojo que han cambiado la orden en Bokeh 2.0.0\n1 2 3 4 5 6  from bokeh.plotting import figure from bokeh.layouts import row p = figure() p1 = figure() # y aqui la magia show(row(p,p1))   La verdad que es bastante intuitivo. Aunque no esté 100% contento con las gráficos, aprender la librería, hacer cositas mas o menos chulas, y poder tirar de ella en el futuro, pinta muy bien.\nLo cierto es que, además, le estoy cogiendo tirria al dark mode XD.\nGráficos Madrid  Andalucía  M vs A (misma gráfica)  M vs. A (gráficas diferentes) Aquí estña el último! Creo que es el que mejor ha quedado, donde he puesto mejor los hovers, y además he ocultado la toolbar si no estás sobre el gráfico!\n Data Source Todos los datos están obtenidos de Numeroteca COVID-19\nLibrerías usadas  Bokeh Pandas  ","description":"Graficos sobre COVID19 usando bokeh","id":39,"section":"posts","tags":["COVID19","Python","Visualization"],"title":"COVID19 + Bokeh: Madrid vs. Andalucía","uri":"https://thebooort.github.io/posts/covid_bokeh_madrid_vs_and/"},{"content":"Este post comienza una serie sobre visualización del COVID-19. La motivación para hacerla no es otra que ocupar los ratos muertos de la cuarentena para aprender a crear gráficos interactivos con Bokeh.\nY de paso ofrecer un vistazo objetivo a los datos que nos llegan sobre la enfermedad. Sé que suena un poco raro pero ya que voy a estar dándole vueltas al tema, tener este enfoque objetivo me ayuda a sobrellevarlo.\nEn cuanto depure los códigos los subo a github y tendréis el enlace aquí:\nCOVID19- El caso de Andalucía Comenzamos con el gráfico que primero vino a mi cabeza. Mi idea era obtener los datos de las provincias andaluzas, pero, hasta la fecha de subir este post, no encontraba más que los de las comunidades autónomas.\nEn este caso, el ejercicio me llevó a comenzar de cero con Bokeh. Un gráfico tan sencillo como ese sirve para iniciarte en los aspectos básicos de la librería, de los cuales destaco:\n  Hovers: son las capas de visualización que te dan información sobre un punto cuando pasas el raton por encima\n  Curdoc: importada desde bokeh.io, te permite cambiar el estilo del grafico (a modo dark jeje)\n  Manejo de TOOLS: que indica que herramientas quieres que presente el gráfico interactivo para ser manipulado. En mi caso:\n  1 2  TOOLS = \u0026#39;crosshair,save,pan,box_zoom,reset,wheel_zoom\u0026#39; plot = figure(tools=TOOLS)    Plot de datos temporales: Hubo que hacer algún reajuste sobre el tipo de datos y luego indicarle a bokeh que estabamos trabajando con esto en su creacion:  1  plot = figure(x_axis_type=\u0026#39;datetime\u0026#39;, tools=TOOLS)   Finalmente opté por crear mi primer gráfico con puntos y líneas.\nGraficos  Data Source Todos los datos están obtenidos de Numeroteca COVID-19\nLibrerías usadas  Bokeh Pandas  ","description":"Graficos sobre COVID19 usando bokeh","id":40,"section":"posts","tags":["COVID19","Python","Visualization"],"title":"COVID19 + Bokeh","uri":"https://thebooort.github.io/posts/covid_bokeh/"},{"content":"Hey reader, Bart here! My name is Bartolomé Ortiz, I\u0026rsquo;m a Ph.D. student of Computer Science at Granada University (Spain). My background is Applied Mathematics (BSc in Maths, MSc in Maths and Physics). I\u0026rsquo;m currently researching Recommender Systems for complex objects, specifically with applications in nutrition, diets, and microbiome. I work as a predoctoral researcher in a European Project (Stance4health). I\u0026rsquo;m an advocate of open science and open software. In my free time, I love to talk about math, data science, computer science, skepticism and biology. I love to give talks about those things. I\u0026rsquo;m the co-host of a science podcast: The Fluxions. I also love trail running, hiking and martial arts. ","description":"There is not such thing as boring mathematics","id":43,"section":"","tags":null,"title":"About","uri":"https://thebooort.github.io/about/"}]